{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Module 4 - Margin and Ensemble Classifiers\n",
    "\n",
    "Before the popularization of deep learning, many applied automatic classification algorithms were variations on ensemble or margin classifiers. Both types of classifiers operate on pre-defined features. As we will see later, this is fundamentally different from neural networks which can learn directly from the images. For now, we will make use of the features pulled from SPC data in the last module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import skimage\n",
    "import sklearn\n",
    "import sys\n",
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The important new toolkit we are importing here is *sklearn*, short for scikit-image. It contains most of the tools we will use to explore margin and ensemble classifiers.\n",
    "\n",
    "For all the techinques discussed in the rest of this module, we will make use of the same features we computed before. We will also need to divide it into seperate sets for training and testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class 0 : Ciliate 01 , num samples: 424\n",
      "class 1 : Glob , num samples: 16\n",
      "class 2 : Acantharea , num samples: 24\n",
      "class 3 : Poop 01 , num samples: 63\n",
      "class 4 : Bubble , num samples: 331\n",
      "class 5 : Sphere 01 , num samples: 38\n",
      "class 6 : Bad Seg , num samples: 249\n",
      "class 7 : Akashiwo , num samples: 447\n",
      "class 8 : Polykrikos , num samples: 331\n",
      "class 9 : Nauplius , num samples: 336\n",
      "class 10 : Ellipse 01 , num samples: 43\n",
      "class 11 : Lingulodinium , num samples: 653\n",
      "class 12 : Protoperidinium Feeding , num samples: 11\n",
      "class 13 : Ciliate 02 , num samples: 14\n",
      "class 14 : Chain 01 , num samples: 352\n",
      "class 15 : Diatom chain , num samples: 60\n",
      "class 16 : Round 01 , num samples: 65\n",
      "class 17 : Phyto Mix 01 , num samples: 895\n",
      "class 18 : Ciliate 03 , num samples: 24\n",
      "class 19 : Ceratium fusus , num samples: 1011\n",
      "class 20 : Avocado 01 , num samples: 62\n",
      "class 21 : Ceratium furca two , num samples: 38\n",
      "class 22 : Cochlodinium , num samples: 332\n",
      "class 23 : Red Eye , num samples: 44\n",
      "class 24 : Prorocentrum Skinny , num samples: 472\n",
      "class 25 : Penate , num samples: 135\n",
      "class 26 : Star 01 , num samples: 34\n",
      "class 27 : Dual Blobs , num samples: 81\n",
      "class 28 : Ceratium furca , num samples: 1411\n",
      "class 29 : Blob Group , num samples: 11\n",
      "class 30 : Prorocentrum , num samples: 1112\n",
      "class 31 : Bubble Like , num samples: 361\n",
      "class 32 : Skinny Mix 01 , num samples: 2052\n",
      "class 33 : Aggregate Mix , num samples: 7289\n",
      "class 34 : Protoperidinium sp , num samples: 198\n",
      "class 35 : Blobble , num samples: 400\n",
      "class 36 : Spear 01 , num samples: 524\n",
      "class 37 : Sand , num samples: 740\n",
      "Total samples: (20683, 71)\n"
     ]
    }
   ],
   "source": [
    "# load in the data. first get all the file paths\n",
    "ptf = glob.glob(os.path.join(\"/media/storage/image_data/SPC_data/manual_labels_features/\",\"*.csv\"))\n",
    "\n",
    "# make an empty list for the data. \n",
    "# the first entry will be the class, the rest the features\n",
    "data = np.empty((0, 71))\n",
    "\n",
    "# we will also create a flag and a listto give the labels a numeric value\n",
    "flag = 0\n",
    "cls_names = []\n",
    "for line in ptf:\n",
    "\n",
    "    # read in the data, but skip the image path\n",
    "    temp = np.genfromtxt(line, usecols=range(1,71),delimiter=\",\")\n",
    "    \n",
    "    # create an array to hold represent the labels\n",
    "    temp2 = flag*np.ones([temp.shape[0],])\n",
    "    \n",
    "    # add them together\n",
    "    temp = np.column_stack((temp2, temp))\n",
    "    \n",
    "    # for now, we will ignore any classes with fewer than 10 samples\n",
    "    if 10 < temp.shape[0]:\n",
    "        # append them to the whole data array\n",
    "        data = np.concatenate((data, temp), axis=0)\n",
    "\n",
    "        # create a list of the names of the categories and the associated numbers\n",
    "        name = line.split('/')[-1].split('_')[0]\n",
    "        print(\"class\", str(flag), \":\", name, \n",
    "              \", num samples:\", str(temp.shape[0]))\n",
    "        cls_names.append((flag, name))\n",
    "        flag+=1\n",
    "\n",
    "print(\"Total samples:\", str(data.shape))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Margin classifiers\n",
    "\n",
    "A margin classifier seperates data in a space by assigning a distance between each point and the decision boundary. Imagine that we have just 2 features, $x_{1}$ and $x_{2}$, to seperate two classes. We can plot the points in a plane and find a line that seperates them.\n",
    "\n",
    "\n",
    "<figure>\n",
    "    <img src=\"https://upload.wikimedia.org/wikipedia/commons/b/b5/Svm_separating_hyperplanes_%28SVG%29.svg\">\n",
    "    <figcaption>\n",
    "        Points and hyperplanes. Courtesy: ZackWeinberg, via Wikipedia\n",
    "    </figcaption>\n",
    "</figure>\n",
    "        \n",
    "\n",
    "The line labeled $H_{3}$ is the best linear discriminant of this data. \n",
    "\n",
    "A classic and widely used margin classifier is the *support vector machine* (SVM). SVMs search a space, definied by the features, to find the optimal seperating hyperplane (ie a plane in many dimensions). In the example above, it would iteratively try many lines such as $H_{1}$ and $H_{2}$ before eventually settling on a particular plane.\n",
    "\n",
    "We will use the implimentation in SKLearn, svc -- a class that contains all functions need to train, test, and deplpoy a SVM. Do note however, that fitting a SVM is computationally expensive and scale quadratically. In other words, training an SVM with O(10k) samples becomes really time consuming and memory hungry.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
