{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 5 - Deep neural networks and feature extraction\n",
    "Neural networks (NN) belong to an entirely different family of learning algorithms called representation learning. Such classifiers learn what they think is most salient about a class directly from the data. Until recently computers were not powerful enough to operate directly on dense data such as images. Instead, researchers used NNs with hand-engineered features as *de facto* feature selectors. \n",
    "\n",
    "The past ten years have seen a confluence of advances in computer processing and labeled datasets. Together, these have led to renewed interest and rapid development in NN algorithms. The underlying mechanism remains similar: feed the network labeled examples, see how it does, adjust paramters, and repeat. This process is done many millions of times to tune weights in a network.\n",
    "\n",
    "There are many specific architectures of neural networks. For most of this tutorial we will focus on using *deep residual networks* or ResNets. ResNets are a refinement of general Convolutional Neural Networks(CNNs) that allow for more efficent training. In essence, there are two distinct phases of a ResNet: feature extraction and classification. The earliest layers of a network are filters that are used to find local regions fitting some pattern in the image.\n",
    "\n",
    "It turns out, these early fitlers are quite general. That is, they do not change very much regardless of the dataset used for training. We can exploit that fact to use a pre-trained network as a feature extractor. So rather then spending lots of time engineering our our features, we will use a ResNet to pull out the information for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.autograd import Variable\n",
    "from sklearn import ensemble\n",
    "from sklearn import preprocessing\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import glob\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "sys.path.insert(1, os.path.join(os.getcwd(),'computer-vision-workshop/utilities'))\n",
    "from internet_utils import get_json_url\n",
    "from custom_torch_utils import ImageFolderWithPaths\n",
    "from display_utils import make_confmat\n",
    "\n",
    "DATASET_PATH = \"/groups/cv-workshop/SPC_manual_labels\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have imported a lot of new stuff here. Most of it is related to pytorch, the library we will be using for running the ResNet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a model and preprocessing data\n",
    "\n",
    "Pytorch ships comes with many preloaded models. Here we will call up ResNet18 and use it with predefinied weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a pretrained network\n",
    "resnet18 = models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This version of resnet has 18 layers and has been already trained with ImageNet. [ImageNet](http://www.image-net.org/) is a huge labeled image dataset maintained by the Stanford Vision Lab. Each image is associated with a noun describing what is in the picture. Each noun has thousands of labeled images assocaited with it. ImageNet is ubiquitious in computer vision research and is used to train many standard models. \n",
    "\n",
    "When ResNet18 is trained with ImageNet, it is tuned for *generic object classification* -- sorting cats from dogs, trucks from cars, etc. This particular version was trained with 1000 generic object classes.\n",
    "\n",
    "Now let us take our diatom chain image and put it through the net and see what we get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the image\n",
    "ptf = glob.glob(os.path.join(os.getcwd(), 'computer-vision-workshop/assets', 'SPC*'))\n",
    "\n",
    "# we will load images with Python Image Library (PIL). \n",
    "img = Image.open(ptf[0])\n",
    "\n",
    "# print the image dimensions\n",
    "print(\"image size:\", img.size)\n",
    "\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the colors look different. PIL loads the color channels in the familiar RGB order, not the BGR order of OpenCV.\n",
    "\n",
    "Also notice that the image dimensions are not square. For margin and ensemble classifiers, this is not an issue. But NN require that everything be the same size as it is put into the system. This is a consequence of the underlying math that governs these systems: for the filters to work, they must be applied to data of the same dimension.\n",
    "\n",
    "To handle this, Pytorch includes an image transform class to put together the *tensors* needed to run through the network. A tensor is a multidimensional matrix with the pre-defined dimensions needed to optimize the speed of training and exectution of a NN. \n",
    "\n",
    "ResNet18 expects input tensors to have the shape [batch_dimension, channel, height, width]. \n",
    "\n",
    "* *batch_dimension* is the number of images to be processed at once. This size is usually a multiple of 2. The maximum size is limited by the available hardware.\n",
    "* *channel* is the number of color channels, usually 3. If using gray scale images with a pretrained network, each image must be replicated into 3 channels. \n",
    "* *height* and *width* are the image width and height. This dimension is also standardized according to the network architecture. It is also usually a multiple of 2. Images that are not of this shape need to be resized accordingly.\n",
    "\n",
    "For now, we need to load a tensor of a single image with the dimensions [1, 3, 224, 224]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the preprocessing transform\n",
    "# this first part normalizes the color channels for ImageNet. If this is not done,\n",
    "# the classifier will get confused.\n",
    "normalize = transforms.Normalize(\n",
    "   mean=[0.485, 0.456, 0.406],\n",
    "   std=[0.229, 0.224, 0.225]\n",
    ")\n",
    "\n",
    "# this is where the image is reshaped into the appropriate tensor dimensions\n",
    "preprocess = transforms.Compose([\n",
    "   transforms.Resize((224, 224)),\n",
    "   transforms.ToTensor(),\n",
    "   normalize\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the preprocessing is defined, we can make our image fit into the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_tensor = preprocess(img)\n",
    "\n",
    "# print out the size\n",
    "img_tensor.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image is now a torch tensor. But it is not quite ready to go into the network. It is still missing the batch size dimension. Since only this one images is going through, use *unsqueeze*, a Pytorch method, to add a dummy dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the dummy dimension at position 0\n",
    "# note that unsqueeze works in place. we do not need to copy the matrix\n",
    "img_tensor.unsqueeze_(0)\n",
    "\n",
    "img_tensor.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the tensor has the right size to put through the network.\n",
    "\n",
    "## Run an image through ResNet\n",
    "\n",
    "Putting an image or set of image through a trained network is known as a *forward pass*. Since the network is already trained, we are not concerned with tuning the network via *back propagation*. We will get there later. \n",
    "\n",
    "For now, the image can be passed through the network and we can see what the label is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass it through the ResNet18 and record the output\n",
    "out = resnet18(img_tensor)\n",
    "\n",
    "# print out the size of the resulting tensor\n",
    "out.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting tensor has 1000 entries, one for each class the ResNet was trained with. This output corresponds to the probability of the image belonging to each of the classes. To print a few of them, check out the data array of the tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out a few of the outputs from the last fully connected layer.\n",
    "out.data.numpy()[0][0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final class is just the maximum value of the array. But taking the max will just give us the position. We need the ImageNet key of classes. Here we grab the ImageNet class index as a json document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unpack json from the following url\n",
    "label_url = \"https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json\"\n",
    "\n",
    "# read the dictionary\n",
    "label_dict = get_json_url(label_url)\n",
    "\n",
    "# make a list of the first few labels and print them\n",
    "first_labs = [(item, label_dict[item][1]) for item in list(label_dict.keys())[0:10]]\n",
    "first_labs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the first 10 labels in the ImageNet data set. Those look pretty familiar! Now let's see what ResNet called the diatom chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the label\n",
    "print(\"ResNet18 sez: \", label_dict[str(out.data.numpy().argmax())][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our great and powerful NN thinks this diatom chain is a bucket. \n",
    "\n",
    "The point here is that the network is not tuned to think about plankton data. In the next module, we will retrain the network to understand plankton data. \n",
    "\n",
    "### A warning about image dimensions\n",
    "\n",
    "Lets see what the diatom chain looks like when it is resized to [224 x 224]. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resize the chain to 224x224\n",
    "img_res = img.resize((224,224))\n",
    "\n",
    "img_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the image looks squished. By resizing the images, we lose some of the scale that we human rely on to identify these organisms. In principle, the computer does not care about the scale. \n",
    "\n",
    "Think carefully when training and testing NN about how the preprocessing will affect the input data. If you think it is important, the aspect can be preserved. There are also methods to re-insert that information in the network.\n",
    "\n",
    "\n",
    "## Extracting features from the ResNet\n",
    "\n",
    "Instead of asking the pretrained model to classify the image, we can use it to pull out features. Here, we will read out the weights associated with the final hidden layer of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, define a copy of the network, but remove the last layer)\n",
    "feat_extractor = nn.Sequential(*list(resnet18.children())[:-1])\n",
    "\n",
    "# Activate evaluation mode\n",
    "feat_extractor.eval()\n",
    "\n",
    "# pump the preprocessed image through the network\n",
    "feats = feat_extractor(img_tensor)\n",
    "\n",
    "# get the tensor from the end of the truncated network\n",
    "feats = feats.data\n",
    "\n",
    "# print the dimensions\n",
    "feats.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tensor has 512 entries corresponding to the weights on the final layer of the network. To collapse this to a single dimension and use as an array, simply flatten it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the torch tensor to np array\n",
    "feats = np.ndarray.flatten(feats.numpy())\n",
    "\n",
    "feats.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have taken the image, run it throught the ResNet trained on ImageNet, and produced a vector of 512 features. These feature can now be used to train a second stage classifier. Basically, we have reduced all the hand engineered feature extraction to just a few lines of code.\n",
    "\n",
    "## Running all the data through\n",
    "\n",
    "Now we need to repeat the process for all the images we want to play with. Our expected output will be the # images by # weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch has a bunch of dataloading utilities built in. This custom loader adds to their ImageFolder utility to have it \n",
    "# return file paths so we can observe the output.\n",
    "# It assumes that the images are loaded as \"{DATASET_PATH}/{class_name}/{image_id}.ext\"\n",
    "dataset = ImageFolderWithPaths(DATASET_PATH, preprocess)\n",
    "\n",
    "# We can use the transform to set up a block of image for the GPU to process\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=4)\n",
    "\n",
    "# This gets a single batch of images\n",
    "images, labels, paths = next(iter(loader))\n",
    "\n",
    "# print the shape of the tensors\n",
    "print(\"images:\", images.shape)\n",
    "print(\"labels:\", labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These inputs were transformed the same way as above, but now the first dimension is 4. This means there are 4 images stacked on top of eachother. This is the *batch_size* and dictates how many images are passed to the network at once. \n",
    "\n",
    "*paths* is a tuple with the path to each file in it. This will be used to view the images later. \n",
    "\n",
    "Now lets see what the output looks like for these 4 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass the images through the network and retrieve the labels\n",
    "feats_small = feat_extractor(images)\n",
    "\n",
    "# just pull out the data and convert to a numpy\n",
    "feats_small = feats_small.data.numpy()\n",
    "\n",
    "# make it into an array and remove extra dimensions\n",
    "feats_small = np.asarray(feats_small)[:, :, 0, 0]\n",
    "\n",
    "# check the dimensions\n",
    "feats_small.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the shape we expect: 4 rows, 512 features corresponding to the weights on each filter. To extract the features from every image requires a for-loop. In we attempted to define a batch that was the entire dataset, the computer would run out of memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the new loader with the bigger batchsize\n",
    "loader_all = torch.utils.data.DataLoader(dataset, batch_size=128)\n",
    "\n",
    "# initalize an empty dictonarty to store the features by image path\n",
    "feat_dict = {}\n",
    "\n",
    "# put the network on the GPU\n",
    "feat_extractor = feat_extractor.cuda()\n",
    "\n",
    "# Activate evaluation mode\n",
    "feat_extractor.eval()\n",
    "\n",
    "# tell the network not to compute gradients since we aren't training\n",
    "with torch.no_grad():\n",
    "    \n",
    "    # use the tqdm module to monitor the progress of the extractor\n",
    "    with tqdm(loader_all, desc=\"Evaluating\") as t:\n",
    "        \n",
    "        # iterate over each batch of 128 in the loader\n",
    "        for inputs, labels, paths in t:\n",
    "            \n",
    "            # put the images onto the GPU\n",
    "            inputs = inputs.cuda()\n",
    "            \n",
    "            # extract the features\n",
    "            feats_temp = feat_extractor(inputs)\n",
    "            \n",
    "            # bring output tensor back onto CPU and collapse extra dimensions\n",
    "            feats_temp = feats_temp.cpu().data.numpy()[:, :, 0, 0]\n",
    "            \n",
    "            # put into a temp dictionary\n",
    "            temp_dict = {paths[ii]: feats_temp[ii, :] for ii in range(len(paths))}\n",
    "            \n",
    "            # update the output\n",
    "            feat_dict.update(temp_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*feat_dict* is organized by the file path of each image. The images have the 512 features associated with them. Again, these features are in many ways akin to those we hand-engineered earlier. But these are defined by a computer trained for generic object classification. \n",
    "\n",
    "## Train a RF model\n",
    "\n",
    "With the ResNet 18 features in hand, we can go ahead and train another Random Forest (RF) using the new features. First, we need to associate a numeric label with each of the images.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of the unique class names from the dictionary keys\n",
    "ptfs = list(feat_dict.keys())\n",
    "cls_names = [line.split('/')[-2] for line in ptfs]\n",
    "cls_names = list(set(cls_names))\n",
    "cls_names.sort()\n",
    "\n",
    "cls_names = [(ii, cls_names[ii]) for ii in range(len(cls_names))]\n",
    "cls_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now shuffle the feature dictionary and split it into training and test data like in the previous module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle the files paths (ie keys)\n",
    "random.shuffle(ptfs)\n",
    "\n",
    "# compute the index for splitting the data\n",
    "idx = 0.8*len(ptfs)\n",
    "\n",
    "train_ids = ptfs[0:int(idx)]\n",
    "test_ids = ptfs[int(idx)::]\n",
    "\n",
    "# double check\n",
    "print(\"cut off for 80-20 split:\", str(int(idx)))\n",
    "print(\"number of training images:\", str(len(train_ids)))\n",
    "print(\"nubmer of test images:\", str(len(test_ids)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now that we have the train-test split we can train and evaluate a classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the training features\n",
    "train_features = [feat_dict[line] for line in train_ids]\n",
    "train_features = np.asarray(train_features)\n",
    "\n",
    "# get the training labels with the look up\n",
    "train_labels = [[line[0] for line in cls_names if item.split('/')[-2] == line[1]][0] for item in train_ids]\n",
    "train_labels = np.asarray(train_labels)\n",
    "\n",
    "# check to make sure these numbers are right. We expect the training features to be a matrix with \n",
    "# dimensions [n_images x n_features] and the training labels to be a matrix with dimensions [n_images x 1]\n",
    "print(\"train features dim:\", train_features.shape)\n",
    "print(\"train labels dim:\", train_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train a Random Forest with 30 trees. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invoke an instance of the standardizer class and fit it to the training features\n",
    "scale_transform = preprocessing.StandardScaler().fit(train_features)\n",
    "\n",
    "# instantiate the RF\n",
    "rf_clf = ensemble.RandomForestClassifier(n_estimators=100, n_jobs=8, verbose=1)\n",
    "\n",
    "# train it. this will take a little longer because the feature space is bigger\n",
    "rf_clf.fit(scale_transform.transform(train_features), train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the RF is trained, the independent test data can be run through it for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first get the features and labels for the test set\n",
    "# get the training features\n",
    "test_features = [feat_dict[line] for line in test_ids]\n",
    "test_features = np.asarray(test_features)\n",
    "\n",
    "# get the training labels with the look up\n",
    "test_labels = [[line[0] for line in cls_names if item.split('/')[-2] == line[1]][0] for item in test_ids]\n",
    "test_labels = np.asarray(test_labels)\n",
    "\n",
    "# check to make sure these numbers are right. We expect the test features to be a matrix with \n",
    "# dimensions [n_images x n_features] and the test labels to be a matrix with dimensions [n_images x 1]\n",
    "print(\"test features dim:\", test_features.shape)\n",
    "print(\"test labels dim:\", test_labels.shape)\n",
    "\n",
    "# get the mean accuracy across all the classes\n",
    "acc = rf_clf.score(scale_transform.transform(test_features), test_labels)\n",
    "\n",
    "# get the labels for the test set from the classifier\n",
    "preds = rf_clf.predict(scale_transform.transform(test_features))\n",
    "\n",
    "# make a confusion matrix\n",
    "make_confmat(test_labels, preds, acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare result to hand-engineered features\n",
    "\n",
    "The performance of the classifier with the ResNet features is comparable to those from the hand-engineered features. We could probably improve the second stage classifier or attempt to pull more generic features from an earlier layer in the ResNet -- here we took the weights from quite near the end of the network.\n",
    "\n",
    "The relative ease of generating these features is substantial. Rather than spending time figuring out what to measure from the image, we crank them through the ResNet and use the weights. \n",
    "\n",
    "As noted above, we sacrifice some scale information that could be important. Depending on the application, it may be worth the deep features from some generated by hand. \n",
    "\n",
    "Given sufficent data, fine tuning a network (module 6) could be a better option. Likewise, if there is a network trained for a specific task close to yours (such as plankton imaging), those weights might be more informative than those generate by a generic object classifier \n",
    "\n",
    "## Exercises\n",
    "\n",
    "To practice the above techniques extract features and train a RF from the ZooScan data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the dataset path\n",
    "DATASET_PATH = \n",
    "\n",
    "# reset the dataset with the defined preprocessing\n",
    "dataset = ImageFolderWithPaths(DATASET_PATH, preprocess)\n",
    "\n",
    "# instantiate a data loader with the a batchsize of 128\n",
    "loader = torch.utils.data.DataLoader()\n",
    "\n",
    "# Check that the size and shape are right\n",
    "images, labels, paths = \n",
    "\n",
    "# print the shape of the tensors\n",
    "print(\"images:\", images.shape)\n",
    "print(\"labels:\", labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the DataLoader seems to be working, extract the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a loop that saves out all the features for the ZooScan data\n",
    "\n",
    "# initalize an empty dictonary to store the features by image path\n",
    "feat_dict = {}\n",
    "\n",
    "# put the network on the GPU\n",
    "feat_extractor = feat_extractor.cuda()\n",
    "\n",
    "# tell the network not to compute gradients since we aren't training\n",
    "with torch.no_grad():\n",
    "    \n",
    "    # use the tqdm module to monitor the progress of the extractor\n",
    "    with tqdm_notebook(loader_all, desc=\"Evaluating\") as t:\n",
    "        \n",
    "        # for-loop goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After extracting the features, generate a class name key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of the unique class names from the dictionary keys\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the keys are in order, split the data in for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle the files paths (ie keys)\n",
    "random.shuffle(ptfs)\n",
    "\n",
    "# compute the index for splitting the data\n",
    "idx = \n",
    "\n",
    "train_ids = \n",
    "test_ids =\n",
    "\n",
    "# double check\n",
    "print(\"cut off for 80-20 split:\", str(int(idx)))\n",
    "print(\"number of training images:\", str(len(train_ids)))\n",
    "print(\"nubmer of test images:\", str(len(test_ids)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the train and test matricies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the training features\n",
    "train_features = \n",
    "train_features = np.asarray(train_features)\n",
    "\n",
    "# get the training labels with the look up\n",
    "train_labels = [[line[0] for line in cls_names if item.split('/')[-2] == line[1]][0] for item in train_ids]\n",
    "train_labels = np.asarray(train_labels)\n",
    "\n",
    "# check to make sure these numbers are right. We expect the training features to be a matrix with \n",
    "# dimensions [n_images x n_features] and the training labels to be a matrix with dimensions [n_images x 1]\n",
    "print(\"train features dim:\", train_features.shape)\n",
    "print(\"train labels dim:\", train_labels.shape)\n",
    "\n",
    "# first get the features and labels for the test set\n",
    "# get the training features\n",
    "test_features = \n",
    "test_features = np.asarray(test_features)\n",
    "\n",
    "# get the training labels with the look up\n",
    "test_labels = \n",
    "test_labels = np.asarray(test_labels)\n",
    "\n",
    "# check to make sure these numbers are right. We expect the test features to be a matrix with \n",
    "# dimensions [n_images x n_features] and the test labels to be a matrix with dimensions [n_images x 1]\n",
    "print(\"test features dim:\", test_features.shape)\n",
    "print(\"test labels dim:\", test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the information in hand, train and evaluate a Random Forest with 100 trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invoke an instance of the standardizer class and fit it to the training features\n",
    "\n",
    "# instantiate the RF\n",
    "\n",
    "# train it. \n",
    "\n",
    "# get the mean accuracy across all the classes\n",
    "\n",
    "# get the labels for the test set from the classifier\n",
    "\n",
    "# make a confusion matrix\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv-workshop",
   "language": "python",
   "name": "cv-workshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
