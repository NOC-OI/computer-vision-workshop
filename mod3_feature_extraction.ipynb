{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 3 - Feature extraction\n",
    "\n",
    "To train and apply ensemble or margin classifiers like random forests or support vector machines, features must be measured from the images. Feature extraction, like region finding, requires a substantial amount of engineering and time and tweaking for a particular dataset.\n",
    "\n",
    "Feature extraction routines assume that candidate regions have already been identified. In this module we will extract features from an example ROI from the SPC dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import skimage\n",
    "from skimage import filters, morphology, measure, color, feature\n",
    "from scipy import ndimage, interpolate\n",
    "import sys\n",
    "import glob\n",
    "import os\n",
    "from math import pi\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as ptch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the file path of the image in the directory. here grab the diatom chain \n",
    "ptf = glob.glob(os.path.join(os.getcwd(),'computer-vision-workshop/assets', 'SPC*'))\n",
    "\n",
    "# We will grab the first item in the list\n",
    "img = cv2.imread(ptf[0])\n",
    "\n",
    "# check to make sure we got the right thing\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This region is one that we might have extracted using techniques for the previous module. Like when find the regions, we need to generate a binary mask to tell the computer what to focus on.\n",
    "\n",
    "## Get a binary mask\n",
    "\n",
    "To start the mask we will use a new edge detector called a Scharr filter. The Scharr filter is an operator similar to Canny that searchers for edges in the intensity image. It is well suited to finding high frequency edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the image gray\n",
    "img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# compute the edges\n",
    "edges_mag = filters.scharr(img_gray)\n",
    "\n",
    "# see what it looks like\n",
    "plt.figure()\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.imshow(edges_mag, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Scharr filter returns gray scale values. Let's make it binary a setting a threshold that is 3 times the median value in the array. This makes our mask more apt to retain edges that using something like Otsu's method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_med = np.median(edges_mag)\n",
    "edges_thresh = 3*edges_med\n",
    "edges = edges_mag >= edges_thresh\n",
    "\n",
    "# see what it looks like\n",
    "plt.figure()\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.imshow(edges, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That doesn't look too bad. But we want to fill in the boundary and select only the largest one for processing. To start with, we can use morphological operations to seal up some of those holes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these will fill in some of the holes\n",
    "edges = morphology.closing(edges, morphology.square(3))\n",
    "filled_edges = ndimage.binary_fill_holes(edges)\n",
    "\n",
    "plt.figure()\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.imshow(filled_edges, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hole filling has expanded the mask a bit. The region growing effectively got rid of the holes, but bloated the mask. Eroding, another morphological operation, will shrink it down again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_close = morphology.erosion(filled_edges, morphology.square(3))\n",
    "\n",
    "plt.figure()\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.imshow(img_close, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It also got rid of some of the smaller noise. We can now use skimages *label* routine to find all the connected regions in the frame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass the label routine the closed image to register connected regions.\n",
    "label_img = morphology.label(img_close, background=0)\n",
    "lab_img_color = color.label2rgb(label_img, image=img_gray)\n",
    "\n",
    "# plot the whole image and the subregion next to each other\n",
    "plt.imshow(lab_img_color, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The yellow boundardy encompasses the diatom chain and gets most of the spines. Before analyzing it, we need to select only the largest boundary in the frame. Regionprops will extract this information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use region props. But this time, have it retain all the properties it can measure\n",
    "props = measure.regionprops(label_img, img_gray)\n",
    "max_area = 0\n",
    "max_area_ind = 0\n",
    "for f in range(0,len(props)):\n",
    "    if props[f].area > max_area:\n",
    "        max_area = props[f].area\n",
    "        max_area_ind = f\n",
    "\n",
    "ii = max_area_ind\n",
    "\n",
    "# now just display that area with the bounding box to make sure it got the right one.\n",
    "\n",
    "# this selects only the pixels in the labeled image that are in the region with the biggest area\n",
    "bw_mask = (label_img) == props[ii].label\n",
    "\n",
    "plt.figure()\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.imshow(bw_mask, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to actually see the masked image, simply multiply the original array by the binary mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_masked = img_gray * bw_mask\n",
    "\n",
    "plt.figure()\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.imshow(img_masked, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will be region that from which all the features will extracted. \n",
    "\n",
    "## Region properties\n",
    "\n",
    "There are many types of metrics that can be used for classification. We will touch on two sets in particular: morphology and texture. All of these features will be saved to a feature vector that will be saved for later use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an empty feature vector to store everything\n",
    "img_features = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Morphology\n",
    "\n",
    "Morphological features describe the shape of the object. Some are familiar, such as the length, width, or area. Others might be a bit more exotic like the convex hull and eccentricity. \n",
    "\n",
    "regionprops has already computed a lot of this for us. First, let's look at what it got for the major and minor axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab just that biggest region\n",
    "prop = props[ii]\n",
    "\n",
    "# the center and orientation of the region have been measures. That will be used to display the axes\n",
    "yy, xx = prop.centroid\n",
    "angle = prop.orientation\n",
    "\n",
    "# the measurements of the major and minor axis\n",
    "maj_ax = prop.major_axis_length\n",
    "min_ax = prop.minor_axis_length\n",
    "\n",
    "# compute the coordinates of the line segment ends\n",
    "x1 = xx + np.cos(angle)*0.5*maj_ax\n",
    "y1 = yy - np.sin(angle)*0.5*maj_ax\n",
    "x2 = xx - np.sin(angle)*0.5*min_ax\n",
    "y2 = yy - np.cos(angle)*0.5*min_ax\n",
    "\n",
    "# plot it\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(img_masked, cmap='gray')\n",
    "\n",
    "# render the major axis from the centriod\n",
    "ax.plot((xx, x1), (yy, y1), 'r-', linewidth=2)\n",
    "\n",
    "# render the minor axis from the centroid\n",
    "ax.plot((xx, x2), (yy, y2), 'r-', linewidth=2)\n",
    "\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The major and minor axes, while not perfect, give a reasonable representation of the images. \n",
    "\n",
    "It is often convenient to compute *invariant* features, ones that are uneffected by changes in scale, rotation, or translation. Saving such features increases the likelihood that the future machine classifer will be able to recognize another diatom chain. \n",
    "\n",
    "The aspect ratio is insenstive to scale. In the case of the diatom chain, it will indicate that the ROI is long and skinny. Aspect ratio is unitless and scales between 0 and 1. 0 indicates a line, 1 would be a circle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aspect = prop.minor_axis_length/prop.major_axis_length\n",
    "\n",
    "print(\"the aspect is: \", str(aspect))\n",
    "\n",
    "# add it to the feature vector\n",
    "img_features.append(aspect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several area ratios that contain useful information. The first makes use of the convex hull of the object. The convex hull is the smallest convex polygon that contains all the pixels in a binary mask. We can see what a hull looks like using skimage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the convex hull from the mask\n",
    "hull = morphology.convex_hull_image(bw_mask)\n",
    "\n",
    "# now plot the original mask next to the convex hull\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "\n",
    "ax[0].imshow(bw_mask, cmap='gray')\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "\n",
    "ax[1].imshow(hull, cmap='gray')\n",
    "ax[1].set_xticks([])\n",
    "ax[1].set_yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ratio of the area of the mask to the convex hull is indicative of how spiny an object is. region_props prints this as the solidity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the data type to float to ensure we get a decimal out\n",
    "sol = prop.solidity\n",
    "\n",
    "# add it to the feature vector\n",
    "img_features.append(sol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ratio of the area inside the region to the perimeter is also indicative of the spininess of the object. It is in essence a surface area-to-volume metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area3 = prop.area.astype(np.float64)/(prop.perimeter*prop.perimeter)\n",
    "\n",
    "# add it to the feature vector\n",
    "img_features.append(area3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "regionprops computes the extent of the image as a ratio of the pixels in the regions to the pixels in the bounding box. This is another indicator of the how solid the object is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area2 = prop.extent\n",
    "\n",
    "# add it to the feature vector\n",
    "img_features.append(area2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several other automatically features that regionprops computes. We will not cover them in any detail here. Some may or may not be useful depending on the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the area\n",
    "area = prop.area\n",
    "\n",
    "# the eccentricity of an ellipse with the same second-moments as the masked region\n",
    "ecc = prop.eccentricity\n",
    "\n",
    "# diameter of a circle with the same area as the object\n",
    "esd = prop.equivalent_diameter\n",
    "\n",
    "# the euler number is 1 minus the number of holes in the object. For this diatom chain, it will be 1.\n",
    "en = prop.euler_number\n",
    "\n",
    "# add these to the feature vector\n",
    "img_features.extend([area, ecc, esd, en])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image moments are weighted averages of the image's pixel intensities. Hu moments are a set of 7 particular moements that approximately invarient across common image transformation like translation, scaling, and rotation. That means that the ROI can be warped in several ways and retain the same Hu moments. This property is very useful for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the hu moments to the feature vector\n",
    "img_features.extend(prop.moments_hu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Texture\n",
    "\n",
    "To this point, the features only contain information about the shape of the object. We can extract information about the texture using the intensity values in the masked image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a histogram of the pixel intensities assuming the image has gray scale values between 0 and 256\n",
    "img_hist = np.histogram(img_masked, 256)\n",
    "\n",
    "# numpy's histogram returns the bin sizes, too. only look at the counts in each gray level\n",
    "img_hist = np.asarray(img_hist[0]).astype(np.float64)\n",
    "\n",
    "# since most of the image is black and we only care about the stuff in the region, set the bin for black pixels=0\n",
    "img_hist[0] = 0\n",
    "\n",
    "# normalize to scale it from zero to one\n",
    "img_hist = img_hist/img_hist.sum()\n",
    "\n",
    "plt.bar(range(0, 256),img_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This histogram is the probability distribution of gray scale values in our ROI. We can now compute some feature from this PDF that are indicative of texture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a list of the normalized pixel values from 0 to 256.\n",
    "vec = np.arange(0, len(img_hist)).astype(np.float64) / (len(img_hist) - 1)\n",
    "\n",
    "# get the bins with nonzero number of pixels in them.\n",
    "ind = np.nonzero(img_hist)[0]\n",
    "\n",
    "# mean grey value\n",
    "mu = np.sum(vec[ind] * img_hist[ind])\n",
    "\n",
    "# variance in the gray scale values\n",
    "var = np.sum((((vec[ind] - mu)**2) * img_hist[ind]))\n",
    "\n",
    "# standard deviation of the gray scale values\n",
    "std =  np.sqrt(var)\n",
    "\n",
    "# contrast - a number indicating the difference between a pixel it's neighbors over the whole image\n",
    "cont = 1 - 1/(1 + var)\n",
    "\n",
    "# 3rd moment. A metric that indicates the distribution of gray values. \n",
    "# A high 3rd moment indicates that there are more bight pixels. Low 3rd moment says the opposite. A 3rd moment of zero\n",
    "# means there is a roughly equal distribution of light and dark pixels\n",
    "thir = np.sum(((vec[ind] - mu)**3)*img_hist[ind])\n",
    "\n",
    "# Uniformity - how flat the intensities are. If the image a single value, uniformity=1\n",
    "uni = np.sum(img_hist[ind]**2)\n",
    "\n",
    "# Entropy is a measure of randomness in the pixel intenisties. \n",
    "ent = - np.sum(img_hist[ind] * np.log2(img_hist[ind]))\n",
    "\n",
    "# add them all the feature vector\n",
    "img_features.extend([mu, var, std, cont, thir, uni, ent])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These metrics computed from the histogram of intensities contain a great deal of information regarding the gray levels in the ROI. But they contain no information about the relative positions of the gray levels. That is, it might tell us that there is a repeating pattern, but not what the pattern is. \n",
    "\n",
    "The last set of texture metrics we will add to these feature vectors is thus the gray level coocurrence matrix (GLCM). The GLCM contains the probability of pixel pairs having the same value at a defined distance and orientation from each other. \n",
    "\n",
    "skimage's *graycomatrix* will compute such a matrix at a set of defined distances and orientations across a whole image. It will return a 4D matrix with the value at each combination of distances and orientation at every coordinate in the region.\n",
    "\n",
    "This matrix is too big to save as a feature and probably contains redundent information anyway. Using skimage's *greycoprops* will compute summary statistics at each pair of distances and angles. Here we will tell it to comptute 4 summary statistics:\n",
    "\n",
    "1. contrast - the sum of squared differences between the pixel pairs\n",
    "2. dissimilarity - sum of the absolute differences between the pixel pair\n",
    "3. energy - the square root of the sum of the squared probabilities of each pair.\n",
    "4. correlation - the of each pair of values at the distance and angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, define the distances to use (in pixels)\n",
    "dist = [1, 2, 4, 16, 32, 64]\n",
    "\n",
    "# then define the angles (in radians)\n",
    "ang = [0, pi/4, pi/2, 3*pi / 4]\n",
    "\n",
    "# compute the matrix from the masked image\n",
    "pp = feature.graycomatrix(img_masked, distances = dist, \n",
    "                         angles = ang, normed = True)\n",
    "\n",
    "# define an output matrix for the metrics\n",
    "grey_mat = np.zeros([24,2]) \n",
    "\n",
    "# set a flag to move the index of the output matrix\n",
    "flag = 0\n",
    "\n",
    "# tell skimage which metrics to compute and iterate over them\n",
    "grey_props = ['contrast', 'homogeneity', 'energy', 'correlation']\n",
    "for name in grey_props:\n",
    "    \n",
    "    # actually compute the features\n",
    "    stat = feature.graycoprops(pp, name)\n",
    "    \n",
    "    # take the mean and standard deviation of the metrics to further compress them\n",
    "    grey_mat[flag:flag+6,0] = np.mean(stat,1)\n",
    "    grey_mat[flag:flag+6,1] = np.std(stat,1)\n",
    "    flag += 6\n",
    "\n",
    "# Add to feature Vector\n",
    "img_features.extend(grey_mat[:,0])\n",
    "img_features.extend(grey_mat[:,1])\n",
    "\n",
    "print(\"total features: \", str(len(img_features)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have thus converted our diatom ROI into a vector of 72 numbers. When preparing to train and test an ensemble or margin classifier, all the labeled images would be run through such a routine. Indeed, the training and test sets for the next module have already been processed in this way.\n",
    "\n",
    "There are a multitude of possible useful features for such things. Settling on which features are most appropriate for your application takes time and testing. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv-workshop",
   "language": "python",
   "name": "cv-workshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
