{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 2 - Image Segmentation and Region Finding\n",
    "\n",
    "Before the popularization of neural networks, engineers and scientists spent lots of time developing routines to crop out regions of an images. This process of image segmentation and region finding is the first step to classifying images with margin and ensemble classifiers. Once the regions of interest are detected, features can be extracted to train, test, and apply a classifier. \n",
    "\n",
    "These techniques are useful for preprocessing data raw data and generating image metrics that preserve the original, physical scale of the data. It also provides a helpful baseline to compare against deep methods.\n",
    "\n",
    "As with any python application, we first import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import skimage\n",
    "from skimage import filters, morphology, measure, color\n",
    "from scipy import ndimage\n",
    "import sys\n",
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as ptch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have added a new libray to work with: skimage, short for Scikit-Image. This is another image processing toolbox that adds additonal functionality to OpenCV.\n",
    "\n",
    "To start with, we will pull up a raw SPC image. This is what is directly captured on the sensor *in situ*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the file path of the image in the directory\n",
    "ptf = glob.glob(os.path.join(os.getcwd(), 'SHRINK-SPC*'))\n",
    "print(ptf[0])\n",
    "\n",
    "# We will grab the first item in the list\n",
    "img = cv2.imread(ptf[0])\n",
    "\n",
    "# change to \n",
    "\n",
    "# now display it so we can see what we are working with\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This does not look like much. Indeed, most of the full frame image is empty space. But take a closer look. Try taking a subimage from the full frame. Constrain the height between 550 and 700  and the width between 400 and 600."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_sub = img[550:700, 400:600]\n",
    "plt.imshow(img_sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A human can go through and grab everything out of the frame in this manner, but it would be time consuming. Instead, we can use edge detection to find all the objects. \n",
    "\n",
    "## Region finding\n",
    "\n",
    "There are many ways to find regions in an image. The specific method choosen for your data very much depends on the type of images and the background. We will just explore a few here.\n",
    "\n",
    "It is important to note that none of these are completely fool proof. They all require some amount of human effort to emprically set parameters that dictate the behavior of the algorithm. It is important to test the fidelity of the code under many different conditions to ensure that it is behaving as expected.\n",
    "\n",
    "### Thresholding\n",
    "\n",
    "If the image background is uniform enough, setting a binary threshold to find pixels above a certain value might be sufficent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first make a copy of the full image as a gray scale image\n",
    "img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# now look at some of the image parameters. here we will use numpy to compute a few things\n",
    "print(\"the max px: \", str(np.max(img_gray)))\n",
    "print(\"the min px: \", str(np.min(img_gray)))\n",
    "print(\"the mean px: \", str(np.mean(img_gray)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that most of the image is that minimum value or close to it, the mean might be an effective filter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use numpy to select the pixels and scale it\n",
    "img_thresh = np.where(img_gray < np.mean(img_gray), 0., 1.0)\n",
    "\n",
    "# this time plot the whole image and the subregion next to each other\n",
    "fig, ax = plt.subplots(1, 2)  # subplots allow defining two sets of axes\n",
    "ax[0].imshow(img_thresh, cmap='gray') # they can then be accessed numerically\n",
    "ax[1].imshow(img_thresh[550:700, 400:600], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean threholded image is kind of noisy. One option is to empirically pick several other threshold values and see how they work. Fortunately, there is an algorithmic approach. \n",
    "\n",
    "Otsu's method searches for a threshold in the image by examining it's intensity histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make an intensity histogram.\n",
    "# ravel flattens the array, 256 is the number of bins, and [0, 256]\n",
    "plt.hist(img_gray.ravel(), 256, [0, 256])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This displays the number of pixels in each bin. Clearly the most common bin is zero (ie black). This is akin to an intensity histogram you might use in Photoshop or iPhoto to mess with an images color. Zoom in to see more structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zoom in on the intensity plot\n",
    "plt.hist(img_gray.ravel(), 256, [0, 256])\n",
    "plt.ylim([0, 10000])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we see that there are two peaks in the gray scale image. Otsu's method attempts to find a threshold that minimizes the variance of the pixels on either side of the boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the skimage implimentation of Otsu's method\n",
    "thresh = filters.thresholding.threshold_otsu(img_gray)\n",
    "print(\"Otsu threshold: \", str(thresh))\n",
    "\n",
    "# now plot it on the intensity histogram \n",
    "plt.hist(img_gray.ravel(), 256, [0, 256])\n",
    "plt.ylim([0, 10000])\n",
    "plt.axvline(x=thresh, color='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This might be more effective than using the mean. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_otsu = np.where(img_gray < thresh, 0., 1.0)\n",
    "\n",
    "# plot the whole image and the subregion next to each other\n",
    "fig, ax = plt.subplots(1, 2)  # subplots allow defining two sets of axes\n",
    "ax[0].imshow(img_otsu, cmap='gray') # they can then be accessed numerically\n",
    "ax[1].imshow(img_otsu[550:700, 400:600], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While not perfect, Otsu's threshold helped element a lot of the small noise particles. \n",
    "\n",
    "Generally speaking, thresholding will discard a good amount of information. It works pretty well on SPC images because the foreground pixels are so distinct from the dark background. But you can see how difficult it would be to select a value that would grab all the stuff you are interested in a more complex image. \n",
    "\n",
    "### Filtering\n",
    "\n",
    "Another good option for selecting regions in an image is *filtering*. An image filter is a sliding window that is dragged across an image to perform an operation in a neighboorhood around every pixel. A 3x3 median fitler, for example, computes the median value in a 3x3 window around the central pixel. In general, filters have odd numbered dimensions.\n",
    "\n",
    "Akin to acoustics, filtering images can be described mathematically as a 2D convolution. That means the computer can cast the filtering operation as a multiplication in frequency space, rather than iteratively computing a value at every index. The details go beyond the scope of the tutorial.\n",
    "\n",
    "There are many different operations that are done with filtering. In fact, convolutional neural networks make use of filters in the feature extraction phase. Here, we will use them to find edges.\n",
    "\n",
    "#### Edge detection\n",
    "Edges in images can be modeled as high frequency component of the image matrix. In other words, edges tend to be sharp discontinuities in pixel values. In the image we are working with, the pixels containing the plankton are bright and the background is dark. Where the pixels transition from light to dark will show up as an edge in the image.\n",
    "\n",
    "We can exploit this to search for edges. First, try out a *Laplacian filter* -- a filter that computes the second derivative within the window. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a laplacian filter from OpenCV\n",
    "img_laplace = cv2.Laplacian(img_gray, cv2.CV_16UC1, ksize=1)\n",
    "\n",
    "# plot the whole image and the subregion next to each other\n",
    "fig, ax = plt.subplots(1, 2)  # subplots allow defining two sets of axes\n",
    "ax[0].imshow(img_laplace, cmap='gray')\n",
    "ax[1].imshow(img_laplace[550:700, 400:600], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second arguement in the Laplacian command specifies the image depth, or data type of the output. We set it to uint16 to increase the percision of the computation. This is not always necessary, but it helps for a mostly empty image. *ksize* specificies the kernel size. ksize=1 gives us a kernal that look likes this:\n",
    "\n",
    "$$\\left[\n",
    "\\begin{matrix}\n",
    "0 & 1 & 0 \\\\\n",
    "1 & -4 & 1 \\\\\n",
    "0 & 1 & 0 \n",
    "\\end{matrix}\n",
    " \\right] $$\n",
    " \n",
    "The 3x3 neighborhood around each pixel is multiplied by this matrix.The resulting values are all added together to get the output for the index at the center. \n",
    "\n",
    "This output looks better at a birds eye view. Take a closer look the gelatinous region. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select subregion and plot\n",
    "plt.imshow(img_laplace[550:700, 400:600], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have found the outline of all the objects of the in the image. And we are starting to resolve some of the structure in the body of the organism itself.\n",
    "\n",
    "#### Canny edge detector\n",
    "\n",
    "The Canny edge detector is a multistage algorithm for edge detection written by John Canny in 1986. It works very well in many cases and is a common element of an image processors toolbox. There are 4 steps: \n",
    "\n",
    "1. Noise reduction -- generally done with a Gaussian smoothing filter. Basically, make the image a little blurry. \n",
    "2. Finding intensity gradients with a filter similar to the Laplacian.\n",
    "3. Non-maximal suppresion forceses the edges to be thin. This stage outputs a binary images with the range [0 255].\n",
    "4. Hysteresis thresholding determines which edges are real. This is done based on the minimum and maximum values the engineer gives the algorthm. A value above the max is sure to be an edge. A value below the minimum is sure *not* to be an edge. In between the max and the min, the algorithm checks to see if a line segement connects to a sure edge.\n",
    "\n",
    "Setting the both the max and minimum values for the hysteresis thresholding high makes the edge detector more conservative. These thresholds need to be tinkered with empirically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run canny\n",
    "img_canny = cv2.Canny(img_gray, 150, 225)\n",
    "\n",
    "plt.imshow(img_canny, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img_canny[550:700, 400:600], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try changing the max and min values of the threshold to see what you get. \n",
    "\n",
    "### Extracting regions\n",
    "\n",
    "Pulling out smaller regions from an image requries locating the desired objects. There are many algorithms implimented in both OpenCV and skimage to find region in an image. Here, we will illustrate the use of skimage's *label* routine. \n",
    "\n",
    "*label* uses connected component analysis to locate and label regions in an image. It checks each pixel to see how many neighboring pixels are in the foreground. Once it crawls all the pixels in a region, it gives the region a numeric label. The process repeats itself until all pixesl have been considered. \n",
    "\n",
    "To aid the process, it is good practice to use a morphological operators to connect edges and complete outlines. Morhpological opening and closing drag a *structuring element* -- a predefined shape used to probe foreground regions in a binary image.\n",
    "\n",
    "Here we will use morphological closing. This routine will close small dark spots in otherwise complete objects. For this example, let's work with the Otsu thresholded image. We will use a square structuring element 5 pixel edges. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_close = morphology.closing(img_otsu, morphology.square(5))\n",
    "\n",
    "# plot the whole image and the subregion next to each other\n",
    "fig, ax = plt.subplots(1, 2)  # subplots allow defining two sets of axes\n",
    "ax[0].imshow(img_close, cmap='gray')\n",
    "ax[1].imshow(img_close[550:700, 400:600], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now compare the original Otsu mask with the closed one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the whole image and the subregion next to each other\n",
    "fig, ax = plt.subplots(1, 2)  # subplots allow defining two sets of axes\n",
    "ax[0].imshow(img_otsu[550:700, 400:600], cmap='gray')\n",
    "ax[1].imshow(img_close[550:700, 400:600], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these complete masks we can now use skimage's region labeling. It will decide if a pixel belongs in a region by checking its connectivety using 8 neighbors. Consider the following grid:\n",
    "\n",
    "$$\\begin{matrix}\n",
    "0 & 0 & 0 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "0 & 0 & 0 \n",
    "\\end{matrix}$$\n",
    "\n",
    "The pixel in the center is one, but all its neighbors are zero. Since it is not connected to anything else, it will be its own regions.\n",
    "\n",
    "$$\\begin{matrix}\n",
    "1 & 0 & 0 \\\\\n",
    "1 & 1 & 0 \\\\\n",
    "1 & 0 & 0 \n",
    "\\end{matrix}$$\n",
    "\n",
    "The pixel in the center of this grid, however, will be a part of the region defined by the neighbors to it's left. When using 8-neighbor connectivity, any value of 1 in the pixels around the center will register as a contiguous region. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass the label routine the closed image to register connected regions.\n",
    "label_img = morphology.label(img_close, neighbors=8, background=0)\n",
    "lab_img_color = color.label2rgb(label_img, image=img_gray)\n",
    "\n",
    "# plot the whole image and the subregion next to each other\n",
    "fig, ax = plt.subplots(1, 2)  # subplots allow defining two sets of axes\n",
    "ax[0].imshow(lab_img_color, cmap='gray')\n",
    "ax[1].imshow(lab_img_color[550:700, 400:600], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The colors correspond to different labeled regions. We can now feed this to labeled image to skimages *regionprops*, a routine that computes lots of information about a region based on the pixels inside it. For now, we will just use it compute the area -- the total number of pixels in the labeled region -- and the dimensions of a bounding boxes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The last arguement is a list of features to pull out of each region\n",
    "props = measure.regionprops(label_img, img_gray, ['Area', 'BoundingBox'])\n",
    "\n",
    "# The length of this vector correspond to the number of regions found\n",
    "print(\"The number of regions is: \", str(len(props)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is a lot! regionprops grabbed all the little bits of detritus in the image. We can filter by area to only examine the biggest regions in the original image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first open up the original gray scale image as a background \n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(img_gray, cmap = 'gray')\n",
    "\n",
    "# initalize an empty vector to store all the bounding boxes\n",
    "prop_out = []\n",
    "\n",
    "# set the area threshold\n",
    "area_thresh = 500\n",
    "\n",
    "# iterate through the regions\n",
    "for prop in props:\n",
    "    \n",
    "    # only select those with an area bigger than the threshold area\n",
    "    if prop.area > area_thresh:\n",
    "        \n",
    "        # save a list of the big ones\n",
    "        prop_out.append(prop.bbox)\n",
    "        \n",
    "        # Bounding box returns the coordinates of the miniumn and maximum row and column locations as:\n",
    "        # [min_row, min_col, max_row, max_col]\n",
    "        # We can use those values to generate a box in the image.\n",
    "        rect = ptch.Rectangle((prop.bbox[1], prop.bbox[0]), prop.bbox[3] - prop.bbox[1], prop.bbox[2] - prop.bbox[0],\n",
    "                          fill=False, edgecolor='red', linewidth=2)\n",
    "\n",
    "        ax.add_patch(rect)\n",
    "        \n",
    "plt.show()\n",
    "\n",
    "# state how many regions fit the critieria\n",
    "print(str(len(prop_out)), \" regions over \", str(area_thresh), \" pixels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now iterate through each of these and crop out the regions in side the bounding box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through the regions and cut out the pixels from the original image. \n",
    "# first add some padding around the region to make sure the bounding box doesn't cut anything off. \n",
    "# here we will use half the width, effectively doubling the size of the boundary. \n",
    "\n",
    "# first initalize a dictionary to store the output. This will allow us to store arrays of different sizes. \n",
    "roi_out = dict()\n",
    "\n",
    "# define an flag to count off the regions\n",
    "flag = 0\n",
    "\n",
    "for bbox in prop_out:\n",
    "    \n",
    "    # first, get the height and width of the box\n",
    "    width = bbox[3] - bbox[1]\n",
    "    height = bbox[2] - bbox[0]\n",
    "\n",
    "    # now we will define the upper left corner of the box\n",
    "    # make sure the values are integers for indexing with np.floor\n",
    "    yy = bbox[1] - np.floor(width/2)\n",
    "    xx = bbox[0] - np.floor(height/2)\n",
    "    \n",
    "    # force xx and yy to be integers\n",
    "    xx = int(xx)\n",
    "    yy = int(yy)\n",
    "    \n",
    "    # if either of the values are negative, force them to zero\n",
    "    # this makes sure we get regions at the edge\n",
    "    if xx < 0:\n",
    "        xx = 0\n",
    "    \n",
    "    if yy < 0:\n",
    "        yy = 0\n",
    "        \n",
    "    # now extract the region\n",
    "    roi_temp = img_gray[xx:xx+2*width, yy:yy+2*height]\n",
    "    \n",
    "    # create a dictionary key with an f-string\n",
    "    out_str = f\"roi_{flag}\"\n",
    "    \n",
    "    # save it out\n",
    "    roi_out[out_str] = roi_temp\n",
    "    \n",
    "    # make sure to increase the value of the string\n",
    "    flag += 1\n",
    "    \n",
    "print('Created ', str(flag), ' rois')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, loop through the dictonary to see what the results look like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over all the keys (roi names) in the dictonary\n",
    "for kk in roi_out.keys():\n",
    "    # create a new figure\n",
    "    plt.figure()\n",
    "    \n",
    "    # turn off the axis numbers to make it a bit more readable\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    \n",
    "    # label each ROI \n",
    "    plt.title(kk)\n",
    "\n",
    "    # show it in gray scale\n",
    "    plt.imshow(roi_out[kk], cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These ROIs can then be saved iteratively as their own files. This is what is done on the onboard computer of the SPC and in other plankton imaging instruments. This effectively cuts down on the amount of data stored. \n",
    "\n",
    "Other more optically dense types of image, such as benthic images, require more involved technqiues for image segmentation. Edge detection might be effective if the environment is sparse enough. Othewise, texture-based segmentation methods might be effective. And, or course, neural network based region finding might be the best bet. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
