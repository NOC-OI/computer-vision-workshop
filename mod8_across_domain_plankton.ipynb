{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 8 - Using plankton ResNets across domains\n",
    "\n",
    "How well do classifier that have been trained on other plankton data work on new stuff? Different regions of taxanomic tree? Different instruments?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This module will be about playing around with what you already learned.\n",
    "\n",
    "Eric and Martin already trained a model on the ZooScan dataset before the workshop and saved it using [`torch.save`](https://pytorch.org/tutorials/beginner/saving_loading_models.html#save-load-entire-model). Load it and have a look at the structure of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from utilities.custom_torch_utils import ImageFolderWithPaths\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from utilities.display_utils import make_confmat\n",
    "\n",
    "MODEL_FN = \"zooscan.pth\"\n",
    "\n",
    "model = torch.load(MODEL_FN)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strip away the classifier layer (*fc*) to receive a feature extractor (like we did in [Module 5](mod5_resnet_feature_extractor.ipynb)), activate [evaluation mode](https://pytorch.org/docs/stable/nn.html#torch.nn.Module.eval) and [move the model to the GPU](https://pytorch.org/docs/stable/nn.html#torch.nn.Module.cuda)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply the feature extractor (that was trained on ZooScan data) to the SPC data.\n",
    "\n",
    "First, load the SPC dataset.\n",
    "You can use the regular `ImageFolder` or the custom `ImageFolderWithPaths`.\n",
    "Keep in mind, that you will need to apply some *transformations* to fit the image data to the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then PyTorch needs a `DataLoader` that prepares the batches that can be send through the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to send the data from the loader through the network. Keep in mind that (besides the features themselves) you will need to store the label of each image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will collect the calculated features in this list\n",
    "features = []\n",
    "\n",
    "# You may need more lists like these for possible other data\n",
    "\n",
    "# We don't need to calculate gradients\n",
    "with torch.no_grad():\n",
    "    # Show a nice progress bar\n",
    "    with tqdm_notebook(loader, desc=\"Evaluating\") as loader:\n",
    "        for ... in loader:\n",
    "            # Copy the input data to GPU\n",
    "            ...\n",
    "\n",
    "            # Apply the feature_extractor to the input data an\n",
    "            batch_features = feature_extractor(...)\n",
    "            \n",
    "            # Copy the batch_features back to the cpu and convert to a numpy array\n",
    "            batch_features = batch_features.cpu().numpy()\n",
    "            \n",
    "            # Do the same for possible other data (labels, paths, ...)\n",
    "            ...\n",
    "            \n",
    "            # Append the batch data to the list\n",
    "            features.extend(batch_features)\n",
    "            \n",
    "            # Do the same for possible other data\n",
    "            ...\n",
    "        \n",
    "# Convert the collected values to numpy arrays\n",
    "features = np.array(features)\n",
    "\n",
    "# Do the same for possible other data\n",
    "...\n",
    "\n",
    "print(\"Shape of features (N_images, N_features):\", features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[`sklearn.model_selection.train_test_split`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) is another way of splitting data into distinct sets. Use it to split the features (and possible other data):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train, features_test, ... = train_test_split(features, ...)\n",
    "\n",
    "print(\"Training features shape:\", features_train.shape)\n",
    "print(\"Testing features shape:\", features_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instanciate a [RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) and train it on the training features. Decide on reasonable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(...)\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, evaluate the Random Forest Classifier. Look at [accuracy](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html), [precision and recall](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html) and the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = ...\n",
    "\n",
    "acc = ...\n",
    "\n",
    "# show a confusion matrix\n",
    "make_confmat(..., predictions, acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercises\n",
    "\n",
    "- What happens, if you also strip away the AdaptiveAvgPool2d layer (*avgpool*)? Play around with the numbers of layers that are retained in the feature extractor.\n",
    "- Try other classifiers (e.g. [SVM](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html))."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
