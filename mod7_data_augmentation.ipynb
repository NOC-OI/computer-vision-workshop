{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 7 - Data augmentation\n",
    "\n",
    "Often times, the number of images per class in the labeled datasets we work with are limited by a variety of factors. This is sometimes due to natural variablitiy in the population being observed. Other times, due to contraints in terms of human annotation costs. The result is that the distribution of classes in the trainin set is uneven.\n",
    "\n",
    "In principle, having an uneven training set is not an issue if it the disparity between the classes reflects the natural population. But in the ocean, populations can fluctuate so rapidly in time and space, it is sometimes preferable to have a classifier trained on a uniform class distribtution. This in effect biases the classifier to think each type of organism is equally likely to be imaged. \n",
    "\n",
    "Ideally, there would be enough training images that one could simply subsample each class until the distribution is uniform. But as with the SPC training data provided for the workshop, this is often not the case. One solution is to use *data augmentation* to artifically increase the size of certain classes.\n",
    "\n",
    "The process of data augmentation involves poking and proding images to look subtly different using the image transformations described in module 2. Then as the network is being trained, it is shown different versions of the same organism. Caution needs to be exercised when using this procedure. It is very easy to overfit the classifier to your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from PIL import Image, ImageFilter\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import glob\n",
    "import random\n",
    "from tqdm.notebook import trange, tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using OpenCV, we will use pytorch's image transformations. These can be used with the Compose class that is used to convert RBG images to a tensor. Pytorch has several, common built-in transforms. There is also a *lambda* class that allows users to define custom transforms.\n",
    "\n",
    "First, set up a little plotting function that takes two PIL images as input and plots them with certain parameter. This will prevent cutting and pasting a lot of code down below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_transform_imgs(orig_img, trn_img):\n",
    "    \"\"\"\n",
    "    routine to plot two images stacked on top of each other. No axis ticks or labels. Titles hardcoded\n",
    "    :param orig_img: untransformed PIL image\n",
    "    :param trn_img: transformed PIL image\n",
    "    :return :\n",
    "    \"\"\"\n",
    "\n",
    "    # initalize a 2x1 plot\n",
    "    fig, ax = plt.subplots(nrows=2, ncols=1, figsize=(14, 7))\n",
    "    ax[0].set_title('original')\n",
    "    ax[0].imshow(np.asarray(orig_img))\n",
    "    ax[1].set_title('transform')\n",
    "    ax[1].imshow(np.asarray(trn_img))\n",
    "    \n",
    "    # turn off axis ticks in both\\n\",\n",
    "    for xx in ax:\n",
    "        xx.set_xticks([])\n",
    "        xx.set_yticks([])\n",
    "        \n",
    "    # display the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random flips\n",
    "\n",
    "To start, lets look at just one transformation: RandomHorizontalFlip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A random flip transformation\n",
    "rand_flip = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip()\n",
    "    ])\n",
    "\n",
    "# load the image\n",
    "ptf = glob.glob(os.path.join(os.getcwd(), 'computer-vision-workshop/SPC*'))\n",
    "\n",
    "# we will load images with Python Image Library (PIL). \n",
    "img_orig = Image.open(ptf[0])\n",
    "\n",
    "# do the flip\n",
    "test = rand_flip(img_orig)\n",
    "\n",
    "# display it\n",
    "plot_transform_imgs(img_orig, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did the image show up upside down? Try running the block again. RandomHoizontalFlip will only flip an image with a probability of 0.5.\n",
    "\n",
    "The same is true of the RandomVerticalFlip function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# redefine the random flip composer\n",
    "rand_flip = transforms.Compose([\n",
    "    transforms.RandomVerticalFlip()\n",
    "])\n",
    "\n",
    "# do the flip\n",
    "test = rand_flip(img_orig)\n",
    "\n",
    "# display i\n",
    "plot_transform_imgs(img_orig, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformations can be chained together. The probability of a flip is independent in each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# redefine the random flip composer\n",
    "rand_flip = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip()\n",
    "])\n",
    "\n",
    "# do the flip\n",
    "test = rand_flip(img_orig)\n",
    "\n",
    "# display it\n",
    "plot_transform_imgs(img_orig, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Affine\n",
    "\n",
    "Pytorch has a [RandomAffine](https://pytorch.org/docs/0.4.0/torchvision/transforms.html#torchvision.transforms.RandomAffine) module that encompasses many affine transformations:\n",
    "\n",
    "    * rotation\n",
    "    * translations\n",
    "    * scaling\n",
    "    * shearing\n",
    "    \n",
    "To see how they work, we will go through one-by-one starting with rotation, the first argument in the RandomAffine transform. To use it, define the range of allowable angles for the rotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a random rotation \n",
    "rand_affine = transforms.Compose([\n",
    "    transforms.RandomAffine((0, 360))  # (min_angle, max_angle)\n",
    "])\n",
    "\n",
    "# do the flip\n",
    "test = rand_affine(img_orig)\n",
    "\n",
    "# display it\n",
    "plot_transform_imgs(img_orig, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To translate without rotating, set the first argument to 0 and put in an argument for translate. To use it, you define the maximum fraction of the images dimensions to shift it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a random translation\n",
    "rand_affine = transforms.Compose([\n",
    "    transforms.RandomAffine(0, translate=(0.5, 0.5))  # (max_width, max_vertical)\n",
    "])\n",
    "\n",
    "# do the flip\n",
    "test = rand_affine(img_orig)\n",
    "\n",
    "# display it\n",
    "plot_transform_imgs(img_orig, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling artifically zooms in or out on the image. To set it, define a range of allowable scaling factors. A scale factor of 1 keeps the ROI the same size. In the following code block, we randomly scale an image between half size and 1.5 times the size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a random scale transform \n",
    "rand_affine = transforms.Compose([\n",
    "    transforms.RandomAffine(0, scale=(0.5, 1.5))  # (min_scale, max_scale)\n",
    "])\n",
    "\n",
    "# do the flip\n",
    "test = rand_affine(img_orig)\n",
    "\n",
    "# display it\n",
    "plot_transform_imgs(img_orig, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, an image can be sheared by a randomly selected range of values of degrees. You can enter a range or a single value. A single value will be treated as a range defined as (-val, +val)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a random shear \n",
    "rand_affine = transforms.Compose([\n",
    "    transforms.RandomAffine(0, shear=50)  # (min_angle, max_angle)\n",
    "])\n",
    "\n",
    "# do the flip\n",
    "test = rand_affine(img_orig)\n",
    "\n",
    "# display it\n",
    "plot_transform_imgs(img_orig, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the other transforms, these can all be chained together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do a bunch of random transforms \n",
    "rand_affine = transforms.Compose([\n",
    "    transforms.RandomAffine((0, 360),  # (min_angle, max_angle)\n",
    "                            translate=(0.25, 0.25), # (max_width, max_vertical)\n",
    "                            scale=(0.5, 1.5),  # (min_scale, max_scale)\n",
    "                            shear=30  # angle\n",
    "                           )  \n",
    "])\n",
    "\n",
    "# do the flip\n",
    "test = rand_affine(img_orig)\n",
    "\n",
    "# display it\n",
    "plot_transform_imgs(img_orig, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These sorts of transformations are useful for making limited data appear more abundant to a machine classifier during training. Chaining them together can cause the classifier to learn a richer representation of the organisms than would be possible with raw data alone. \n",
    "\n",
    "## Custom Augmentation\n",
    "\n",
    "Depending on your imaging scenario, it can be useful to design a custom augmentation method. The SPC, for example, is a free body imaging system. It has a fixed focal plane and objects can drift in and our of focus. It might be helpful to train the classifier to recogonize an object even it if is blurry.\n",
    "\n",
    "A 2-D Gaussian kernel is a useful model for blur. If we wish to use a Gaussian blur kernel on the data in line with the rest of the data transforms, we can use a *Lambda* function. It will apply a user defined transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First define a Gaussian blur funtion with PIL\n",
    "def gauss_blur(img, max_blur=9):\n",
    "    \"\"\"\n",
    "    Uses PIL to blur an image with a random kernel size\n",
    "    :param img: PIL image to blur\n",
    "    :param max_blur: max width of blur kernel\n",
    "    :return : blurred image\n",
    "    \"\"\"\n",
    "    \n",
    "    # randomly select a blur kernel (ensuring it is an odd number)\n",
    "    amt = np.random.choice(np.arange(1, max_blur, 2))\n",
    "    \n",
    "    # blur the image with PIL\n",
    "    img = img.filter(ImageFilter.GaussianBlur(int(amt)))\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will blur the input image. We can run it byitself in a pytorch transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a gaussian blur \n",
    "rand_affine = transforms.Compose([\n",
    "    transforms.Lambda(lambda x: gauss_blur(x))\n",
    "])\n",
    "\n",
    "# do the flip\n",
    "test = rand_affine(img_orig)\n",
    "\n",
    "# display it\n",
    "plot_transform_imgs(img_orig, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of the SPC, this can make for a more robust classifier that will have to recognize blurry images in the field. Again, a lambda function can be chained with the other affine transforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do a bunch of random transforms \n",
    "rand_affine = transforms.Compose([\n",
    "    transforms.RandomAffine((0, 360),  # (min_angle, max_angle)\n",
    "                            translate=(0.25, 0.25), # (max_width, max_vertical)\n",
    "                            scale=(0.5, 1.5),  # (min_scale, max_scale)\n",
    "                            shear=30  # angle\n",
    "                           ),\n",
    "    transforms.Lambda(lambda x: gauss_blur(x))\n",
    "])\n",
    "\n",
    "# do the flip\n",
    "test = rand_affine(img_orig)\n",
    "\n",
    "# display it\n",
    "plot_transform_imgs(img_orig, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## A word of caution\n",
    "\n",
    "Data augmentation does comes with a few caveats. \n",
    "\n",
    "* It must be done intelligently and reasonably. You do not want to warp your images beyond human recognition -- this would cause the classifier to learn irrevalent features of the data. \n",
    "* It should be done relatively conservatively. Feeding a network too many augmented version of the same image can cause it to overfit to the limited data. That is, the classifier will only learn to distinguish between many variations of the same image. \n",
    "\n",
    "## Exercises\n",
    "\n",
    "Eric and Martin trained the pytorch model 'spc_resnet.pt' offline using these techniques and saved the weights using ['torch.save'](https://pytorch.org/tutorials/beginner/saving_loading_models.html#save-load-entire-model). The data used for the training are stored in '../Data/SPC_resnet18_domain_adapt' with in the usual format. If you go into the directory, you will note that the number of images per class is the same across the board. This was done by subsampling the biggest classes and duplicating the small ones.\n",
    "\n",
    "As the training images are fed to the classifier, the following augmentation procedure was used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the transform \n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomAffine((0, 360), \n",
    "                            translate=(0.25, 0.25),\n",
    "                            scale=(0.5, 1.5), \n",
    "                            shear=30 \n",
    "                           ),\n",
    "    transforms.Lambda(lambda x: gauss_blur(x)),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the trained network with torch and apply it to the validation set and compare it to the output of the exercise from Module 6 where ResNet18 was trained on unaugmented SPC data. Which network performed better on the validation set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the network weights\n",
    "spc_model = torch.load('spc_resnet.pt')\n",
    "\n",
    "# set it to evaluation mode\n",
    "spc_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code block below, go ahead and execute the same evalutation from Module 6: run through the validation set, save the output, and plot it. Compare the performance to the SPC dataset that was unaugmented. What do you notice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv-workshop",
   "language": "python",
   "name": "cv-workshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
