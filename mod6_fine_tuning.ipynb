{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 6 - Fine-tuning ResNet toward plankton data\n",
    "\n",
    "We have seen that a neural network that was trained on a completely plankton-unrelated dataset (like ImageNet) still produces features that allow the classification of plankton data.\n",
    "Now, we can go a step further and *fine-tune* such a network to do plankton classification.\n",
    "This is akin to teaching a person without prior oceanographic experience how to recognize different types of fish, assuming that they are able to recognize other kinds of objects.\n",
    "\n",
    "In practice, CNNs are almost always fine-tuned (and not trained from scratch) for convergence reasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import RandomSampler\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import random\n",
    "\n",
    "sys.path.insert(1, os.path.join(os.getcwd(),'computer-vision-workshop/utilities'))\n",
    "from display_utils import imshow_tensor, make_confmat\n",
    "from split import stratified_random_split\n",
    "\n",
    "TRAINING_PATH = \"/groups/cv-workshop/ZooScan/train\"\n",
    "VALIDATION_PATH = \"/groups/cv-workshop/ZooScan/train\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading and transformation\n",
    "\n",
    "Image datasets can conveniently loaded with [`torchvision.datasets.ImageFolder`](https://docs.pytorch.org/vision/main/generated/torchvision.datasets.ImageFolder.html).\n",
    "It assumes one folder for each class where the images are located.\n",
    "\n",
    "CNNs have a fixed input size. ResNets happen to be trained with 224x244 images. \n",
    "Therefore, we need to make sure that each image has the correct dimensions.\n",
    "`ImageFolder` has a `transform` parameter for that.\n",
    "After resizing, the images need to be converted to a PyTorch [`Tensor`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor).\n",
    "\n",
    "We will use the training set for the training the network and the validation set for the evaluation of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = Compose([\n",
    "    # Resize every image to a 224x244 square\n",
    "    Resize((224,224)),\n",
    "    # Convert to a tensor that PyTorch can work with\n",
    "    ToTensor()\n",
    "])\n",
    "\n",
    "# Images are located at at {dataset_path}/{class_name}/{objid}.jpg\n",
    "dataset_train = ImageFolder(TRAINING_PATH, transform)\n",
    "dataset_val = ImageFolder(TRAINING_PATH, transform)\n",
    "\n",
    "# Make sure that the class names are identical\n",
    "assert dataset_train.classes == dataset_val.classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at the first example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the tensor and the label of the first example\n",
    "tensor, label = dataset_train[0]\n",
    "\n",
    "print(\"Class: {:d} ({})\".format(label, dataset_train.classes[label]))\n",
    "imshow_tensor(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the model\n",
    "\n",
    "We start with a pre-trained ResNet18 model.\n",
    "It was initially trained on ImageNet which happens to contain 1000 classes. However, our plankton dataset contains XXX classes. Therefore, we have to reset the classifier layer to the correct number of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet18(pretrained=True)\n",
    "\n",
    "# get the number of features that are input to the fully connected layer\n",
    "num_ftrs = model.fc.in_features\n",
    "\n",
    "# reset the fully connect layer\n",
    "model.fc = nn.Linear(num_ftrs, len(dataset_train.classes))\n",
    "\n",
    "# if we are using a multiple GPU system with multiple users on it \n",
    "# then we want to balance out which GPU we use. By default we'll\n",
    "# just use GPU0. Randomly pick one to use.\n",
    "\n",
    "gpu_id = random.randint(0,torch.cuda.device_count()-1)\n",
    "print(\"using GPU\", gpu_id)\n",
    "\n",
    "# Transfer model to GPU\n",
    "model = model.cuda(device=gpu_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the optimizer\n",
    "\n",
    "We will train the network using [Stochastic Gradient Descend (SDG)](https://en.wikipedia.org/wiki/Stochastic_gradient_descent).\n",
    "In each iteration, the network parameters are updated in order to minimize a training criterion, in our case the [Cross Entropy](https://en.wikipedia.org/wiki/Cross_entropy) Loss.\n",
    "The better the predictions, the smaller the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_train = torch.utils.data.DataLoader(dataset_train, batch_size=110,\n",
    "                                           shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activate training mode\n",
    "model.train()\n",
    "\n",
    "# Train for 5 epochs\n",
    "for epoch in trange(5, desc=\"Epoch\"):\n",
    "    # tqdm_notebook displays a nice progress bar\n",
    "    with tqdm(loader_train, desc=\"Training Epoch #{:d}\".format(epoch + 1)) as t:\n",
    "        for inputs, labels in t:\n",
    "            # Copy data to GPU\n",
    "            inputs = inputs.cuda(device=gpu_id)\n",
    "            labels = labels.cuda(device=gpu_id)\n",
    "    # for ii, data in enumerate(loader_train, 0):\n",
    "    #     inputs, labels = data\n",
    "    #     inputs = inputs.cuda(device=gpu_id)\n",
    "    #     labels = labels.cuda(device=gpu_id)\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "            # print statistics\n",
    "            t.set_postfix(loss=loss.item())\n",
    "\n",
    "print('Finished Training')\n",
    "# empty the GPU cache memory to free memory for other users\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate\n",
    "\n",
    "Let's see how well our model performs.\n",
    "\n",
    "First, display some examplary images together with their ground-truth and predicted labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Activate evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# A data loader for the validation set with a batch size of 4 for demonstration purposes\n",
    "loader_val = torch.utils.data.DataLoader(dataset_val, batch_size=4, shuffle=True)\n",
    "\n",
    "# Extract one batch\n",
    "images, labels = next(iter(loader_val))\n",
    "\n",
    "# Show images of the batch\n",
    "imshow_tensor(torchvision.utils.make_grid(images))\n",
    "print('Ground truth:', ', '.join('%5s' % dataset_val.classes[labels[j]] for j in range(4)))\n",
    "\n",
    "# Run the batch through the model\n",
    "outputs = model(images.cuda(device=gpu_id))\n",
    "\n",
    "# Collect the predicted classes\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "print('Predicted:', ', '.join('%5s' % dataset_val.classes[predicted[j]]\n",
    "                              for j in range(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we do a thorough evaluation of the whole dataset. In order to do that, we need to run the whole validation set through the network and record the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_true = []\n",
    "labels_predicted = []\n",
    "\n",
    "# Validation data loader with a reasonable batch size\n",
    "loader_val = torch.utils.data.DataLoader(dataset_val, batch_size=110, num_workers=6, shuffle=True)\n",
    "\n",
    "# Activate evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# We don't need to calculate gradients\n",
    "with torch.no_grad():\n",
    "    with tqdm(loader_val, desc=\"Evaluating\") as t:\n",
    "        for inputs_batch, labels_batch in t:\n",
    "            # Copy data to GPU\n",
    "            inputs_batch = inputs_batch.cuda(device=gpu_id)\n",
    "\n",
    "            outputs = model(inputs_batch)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            labels_true.extend(labels_batch.tolist())\n",
    "            labels_predicted.extend(predicted.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = accuracy_score(labels_true, labels_predicted)\n",
    "print(\"Accuracy:\", acc)\n",
    "print(classification_report(labels_true,\n",
    "                            labels_predicted,\n",
    "                            labels=np.arange(len(dataset_val.classes)),\n",
    "                            target_names=dataset_val.classes))\n",
    "# empty the GPU cache memory to free memory for other users\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you know what these scores are?\n",
    "Make yourself familiar with [precision and recall](https://en.wikipedia.org/wiki/Precision_and_recall) and the [F-Score](https://en.wikipedia.org/wiki/F1_score). These are important metrics for the evaluation of a classifier.\n",
    "\n",
    "You may notice that classes with a larger support (number of examples) tend to get higher scores. Can you guess why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "make_confmat(labels_true, labels_predicted, acc, labels=dataset_val.classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully you see a diagonal of true predictions. You may also notice vertical stripes that occur if a wide range of different objects is classified as the same class. This often happens in datasets with a skewed class distribtion where a few classes contain most of the objects. In this case, the classifier learns that it is a relative save bet to predict these majority classes most of the time. Module 7 will take care of this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "- Apply this notebook to the SPC dataset.\n",
    "- Compare the results to the previous classifiers.\n",
    "- What happens if you use a randomly initialized network (`model = models.resnet18(pretrained=False)`)?\n",
    "- Try different [transformations](https://docs.pytorch.org/vision/0.22/transforms.html).\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "In this module, you learned how to use a folder of images to fine-tune a model in PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Visualization of the feature space\n",
    "\n",
    "How are the classes distributed in the feature space?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the model but remove the last layer\n",
    "feat_extractor = nn.Sequential(*list(model.children())[:-1])\n",
    "\n",
    "features = []\n",
    "labels = []\n",
    "# We don't need to calculate gradients\n",
    "with torch.no_grad():\n",
    "    with tqdm(loader_val, desc=\"Evaluating\") as t:\n",
    "        for input_batch, label_batch in t:\n",
    "            # Copy input batch to GPU\n",
    "            input_batch = input_batch.cuda(device=gpu_id)\n",
    "\n",
    "            features_batch = feat_extractor(input_batch)\n",
    "            \n",
    "            features.extend(features_batch.cpu().numpy())\n",
    "            labels.extend(label_batch.cpu().numpy())\n",
    "            \n",
    "features = np.array(features)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We project the features from 512 dimensions to 2 dimensions using [t-SNE](https://lvdmaaten.github.io/tsne/). This will take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE()\n",
    "features_2d = tsne.fit_transform(np.squeeze(features)[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "scat = ax.scatter(features_2d[:,0], features_2d[:,1], c=labels[:1000])\n",
    "cbar = fig.colorbar(scat)\n",
    "cbar.set_ticks(np.arange(len(dataset_val.classes)))\n",
    "cbar.set_ticklabels(dataset_val.classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally, the different classes build clusters in the feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Free Up GPU memory when done\n",
    "\n",
    "GPU memory will remain in use until the kernel stops. Other users might need this memory to run their jobs. You can free all your memory by clicking \"Kernel -> Shutdown Kernel\". You will still be able to see any results you had visualised in your notebook but the state of all variables will be lost. \n",
    "\n",
    "We can also free some memory by running `torch.cuda.empty_cache()` while still keeping our kernel running and not losing any variable state.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty the GPU cache memory to free memory for other users\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "computer-vision-workshop",
   "language": "python",
   "name": "computer-vision-workshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
