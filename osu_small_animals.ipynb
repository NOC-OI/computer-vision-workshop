{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c13e3552-502b-45ee-b53c-8ae3d52b0b0b",
   "metadata": {},
   "source": [
    "# OSU Small Animals \n",
    "[OSU Small Animals](https://lila.science/datasets/ohio-small-animals/) is a benchmark dataset, a static collection of images used to evaluate the capability of various ML approaches, developed from terrestrial camera trap images. This notebook contains pseudocode giving you an idea of what to do as you work to training a classifer on this labeled data. \n",
    "\n",
    "Unlike previous notebooks, this one is very much a skeleton with some extra information to frame your workflow. In other words, you will write most of the code yourself. You can write everything from scratch or treat this as a vibe coding session (i.e. asking your favorite LLM for help). Please ask the instructors lots of questions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e02dc8d-22c5-4dfb-a3e4-dbcbf31100b1",
   "metadata": {},
   "source": [
    "## Common Objects in COntext format\n",
    "[Common Objects in COntext (COCO)](https://link.springer.com/chapter/10.1007/978-3-319-10602-1_48) is one of the classic machine learning benchmark data (62000 citations and counting!). The material below explaining the COCO format is adapted from the [FathomNet Python API Tutorial](https://github.com/fathomnet/fathomnet-py/blob/main/examples/tutorial.ipynb). \n",
    "\n",
    "The original COCO dataset is a large annotated image dataset containing bounding boxes and segmentation masks for 91 categories in 100s of thousands of images. The dataset led to lots of novel AI models and the format has become standard for many python tools. Widely used architectures like Pytorch (what we've been using in this workshop) and [YOLO](https://docs.ultralytics.com/tasks/classify/) have a [prebuilt DataLoader object for COCO](https://docs.pytorch.org/vision/stable/_modules/torchvision/datasets/coco.html). COCO format supports [multiple types of annotations](https://cocodataset.org/#overview) including: classification, bounding box, segmantions, and keypoints. Regardless of type, annotations are organized into a standard format to make it easier to work with. Typically, the annotation files are distributed as json serialized nested dictionaries. The highest level looks like this:\n",
    "\n",
    "```\n",
    "{\n",
    "  \"info\": info,\n",
    "  \"images\": [image],\n",
    "  \"categories\": [category]\n",
    "  \"annotations\": [annotation]\n",
    "}\n",
    "```\n",
    "\n",
    "Each field contains inter-related pieces of information for your model training code to use. `info` is a dictionary that contains some metadata about the entire dataset:\n",
    "\n",
    "```\n",
    "info = {\n",
    "    \"year\": 2023,\n",
    "    \"version\": \"0\",\n",
    "    \"description\": \"Generated by FathomNet\",\n",
    "    \"contributor\": \"FathomNet\",\n",
    "    \"url\": \"https://fathomnet.org\",\n",
    "    \"date_created\": \"2023/02/23\"\n",
    "}\n",
    "```\n",
    "\n",
    "The `images` field consists of a list of `image` objects that specify the file name, image dimensions, permanent url, etc. \n",
    "\n",
    "```\n",
    "image = {\n",
    "    \"id\": 1,\n",
    "    \"width\": 1920,\n",
    "    \"height\": 1080,\n",
    "    \"file_name\": \"754e6a28-a8eb-4cb3-a0b9-3f2d5daacbae.png\",\n",
    "    \"license\": 0,\n",
    "    \"flickr_url\": \"https://fathomnet.org/static/m3/staging/Doc%20Ricketts/images/0861/00_12_12_05.png\",\n",
    "    \"coco_url\": \"https://fathomnet.org/static/m3/staging/Doc%20Ricketts/images/0861/00_12_12_05.png\",\n",
    "    \"date_captured\": \"2016-06-16 00:00:00\"\n",
    "}\n",
    "```\n",
    "\n",
    "The `categories` field is a list of `category` objects organized by numeric ids.\n",
    "\n",
    "```\n",
    "category = {\n",
    "  \"id\": 2,\n",
    "  \"name\": \"Actinernus\",\n",
    "  \"supercategory\": \"\"\n",
    "}\n",
    "```\n",
    "\n",
    "Finally, `annotations` is a list of `annotation` objects that bring it all together. \n",
    "\n",
    "```\n",
    "annotation = {\n",
    "      \"id\": 1,\n",
    "      \"image_id\": 1,\n",
    "      \"category_id\": 2,\n",
    "      \"segmentation\": [],\n",
    "      \"area\": 51200.0,\n",
    "      \"bbox\": [\n",
    "        200.0,\n",
    "        433.0,\n",
    "        256.0,\n",
    "        200.0\n",
    "      ],\n",
    "      \"iscrowd\": 0\n",
    "}\n",
    "```\n",
    "\n",
    "Note that the `id` fields are specific to the list of objects. The example `annotation` object tells us that in the image associated with `image_id` 1, there a bounding box located at position `[200, 433]` with a width of `256` pixels and a heigh of `200` pixels. The label associated with that bounding box is `category_id` 2 which corresponds the anemone \"Actinernus\". If there are not bounding boxes associated with the annotation, the `bbox` and `area` fields are empty. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aadc993-c937-4a9f-8ac5-2766b9c14159",
   "metadata": {},
   "source": [
    "## Explore OSU Small Animals\n",
    "Start by exploring the OSU Small Animals dataset by loading it in, trying to view images, and exploring metadata. The data can be found in `/groups/cv-workshop/ohio_small_animals` with the images in the `Images` subdirectory. The image and annotation data is in the COCO formated json file `osu-small-animals.json`.\n",
    "\n",
    "This dataset is distributed in a modified version of COCO called [COCO Camera Traps](https://lila.science/coco-camera-traps) (CCT) format. CCT is a superset of COCO and is compatible with tools that expect COCO-formatted data. It includes additional metadata relevant to camera trap deployments, namely location. The location field is in the image field described above and can be useful for making training and validation datasets. \n",
    "\n",
    "You may find the [megadetector utility functions](https://github.com/agentmorris/MegaDetector/tree/main/megadetector/utils) helpful for sorting images by their metadata. We have installed those utlities in this compute environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478385da-2cd9-440e-b4e1-e3135dda124b",
   "metadata": {},
   "source": [
    "Some things you might consider doing: \n",
    "- Counting the number of annotations per class\n",
    "- Select categories that have more than 100 labeled examples\n",
    "- Count the number of animals and the number of different categories detected at different locations "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f483d6-b4a8-4dc7-920e-1193494cb653",
   "metadata": {},
   "source": [
    "## Create training and validation\n",
    "In earlier notebooks, you were given premade training and validation datasets. Recall, the training data is used for tuning your model and the validation is held out for testing. Come up with a training and validation split for your model. This could be purely random or based on some relevant metadata. Up to you!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af14fbe5-1728-4b2e-add4-3d2b0b3a56e9",
   "metadata": {},
   "source": [
    "## Train your model\n",
    "Train a classification model based on the COCO formatted data. You can try to write a custom DataLoader to read in COCO data for classification (as opposed to object detection). Or you could try a different architecture all together. [Ultralytics](https://docs.ultralytics.com/tasks/classify/#models), for example, distributes classification models that can read in data from COCO formatted metadata (NB: this is a different framework from Pytorch, the package we were using earlier). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b1c29f-6a27-4004-afd3-0d8b9d21bf3e",
   "metadata": {},
   "source": [
    "## Evaluate your model\n",
    "Use your validation data to measure performance. Think about how these measurements reflect how you choose to split your data. How might you expect this to work in the real world? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9712ecf-9527-42ea-b05f-839bb6fe1c77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv-workshop",
   "language": "python",
   "name": "cv-workshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
