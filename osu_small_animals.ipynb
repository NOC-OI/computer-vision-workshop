{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c13e3552-502b-45ee-b53c-8ae3d52b0b0b",
   "metadata": {},
   "source": [
    "# OSU Small Animals \n",
    "[OSU Small Animals](https://lila.science/datasets/ohio-small-animals/) is a benchmark dataset, a static collection of images used to evaluate the capability of various ML approaches, developed from terrestrial camera trap images. This notebook contains pseudocode giving you an idea of what to do as you work to training a classifer on this labeled data. \n",
    "\n",
    "Unlike previous notebooks, this one is very much a skeleton with some extra information to frame your workflow. In other words, you will write most of the code yourself. You can write everything from scratch or treat this as a vibe coding session (i.e. asking your favorite LLM for help). Please ask the instructors lots of questions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e02dc8d-22c5-4dfb-a3e4-dbcbf31100b1",
   "metadata": {},
   "source": [
    "## Common Objects in COntext format\n",
    "[Common Objects in COntext (COCO)](https://link.springer.com/chapter/10.1007/978-3-319-10602-1_48) is one of the classic machine learning benchmark data (62000 citations and counting!). The material below explaining the COCO format is adapted from the [FathomNet Python API Tutorial](https://github.com/fathomnet/fathomnet-py/blob/main/examples/tutorial.ipynb). \n",
    "\n",
    "The original COCO dataset is a large annotated image dataset containing bounding boxes and segmentation masks for 91 categories in 100s of thousands of images. The dataset led to lots of novel AI models and the format has become standard for many python tools. Widely used architectures like Pytorch (what we've been using in this workshop) and [YOLO](https://docs.ultralytics.com/tasks/classify/) have a [prebuilt DataLoader object for COCO](https://docs.pytorch.org/vision/stable/_modules/torchvision/datasets/coco.html). COCO format supports [multiple types of annotations](https://cocodataset.org/#overview) including: classification, bounding box, segmantions, and keypoints. Regardless of type, annotations are organized into a standard format to make it easier to work with. Typically, the annotation files are distributed as json serialized nested dictionaries. The highest level looks like this:\n",
    "\n",
    "```\n",
    "{\n",
    "  \"info\": info,\n",
    "  \"images\": [image],\n",
    "  \"categories\": [category]\n",
    "  \"annotations\": [annotation]\n",
    "}\n",
    "```\n",
    "\n",
    "Each field contains inter-related pieces of information for your model training code to use. `info` is a dictionary that contains some metadata about the entire dataset:\n",
    "\n",
    "```\n",
    "info = {\n",
    "    \"year\": 2023,\n",
    "    \"version\": \"0\",\n",
    "    \"description\": \"Generated by FathomNet\",\n",
    "    \"contributor\": \"FathomNet\",\n",
    "    \"url\": \"https://fathomnet.org\",\n",
    "    \"date_created\": \"2023/02/23\"\n",
    "}\n",
    "```\n",
    "\n",
    "The `images` field consists of a list of `image` objects that specify the file name, image dimensions, permanent url, etc. \n",
    "\n",
    "```\n",
    "image = {\n",
    "    \"id\": 1,\n",
    "    \"width\": 1920,\n",
    "    \"height\": 1080,\n",
    "    \"file_name\": \"754e6a28-a8eb-4cb3-a0b9-3f2d5daacbae.png\",\n",
    "    \"license\": 0,\n",
    "    \"flickr_url\": \"https://fathomnet.org/static/m3/staging/Doc%20Ricketts/images/0861/00_12_12_05.png\",\n",
    "    \"coco_url\": \"https://fathomnet.org/static/m3/staging/Doc%20Ricketts/images/0861/00_12_12_05.png\",\n",
    "    \"date_captured\": \"2016-06-16 00:00:00\"\n",
    "}\n",
    "```\n",
    "\n",
    "The `categories` field is a list of `category` objects organized by numeric ids.\n",
    "\n",
    "```\n",
    "category = {\n",
    "  \"id\": 2,\n",
    "  \"name\": \"Actinernus\",\n",
    "  \"supercategory\": \"\"\n",
    "}\n",
    "```\n",
    "\n",
    "Finally, `annotations` is a list of `annotation` objects that bring it all together. \n",
    "\n",
    "```\n",
    "annotation = {\n",
    "      \"id\": 1,\n",
    "      \"image_id\": 1,\n",
    "      \"category_id\": 2,\n",
    "      \"segmentation\": [],\n",
    "      \"area\": 51200.0,\n",
    "      \"bbox\": [\n",
    "        200.0,\n",
    "        433.0,\n",
    "        256.0,\n",
    "        200.0\n",
    "      ],\n",
    "      \"iscrowd\": 0\n",
    "}\n",
    "```\n",
    "\n",
    "Note that the `id` fields are specific to the list of objects. The example `annotation` object tells us that in the image associated with `image_id` 1, there a bounding box located at position `[200, 433]` with a width of `256` pixels and a heigh of `200` pixels. The label associated with that bounding box is `category_id` 2 which corresponds the anemone \"Actinernus\". If there are not bounding boxes associated with the annotation, the `bbox` and `area` fields are empty. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aadc993-c937-4a9f-8ac5-2766b9c14159",
   "metadata": {},
   "source": [
    "## Explore OSU Small Animals\n",
    "Start by exploring the OSU Small Animals dataset by loading it in, trying to view images, and exploring metadata. The data can be found in `/groups/cv-workshop/ohio_small_animals` with the images in the `Images` subdirectory. The image and annotation data is in the COCO formated json file `osu-small-animals.json`.\n",
    "\n",
    "This dataset is distributed in a modified version of COCO called [COCO Camera Traps](https://lila.science/coco-camera-traps) (CCT) format. CCT is a superset of COCO and is compatible with tools that expect COCO-formatted data. It includes additional metadata relevant to camera trap deployments, namely location. The location field is in the image field described above and can be useful for making training and validation datasets.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36d7184a-5b71-411a-8fe5-086957e1ebe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "from megadetector.utils.ct_utils import is_list_sorted\n",
    "from megadetector.utils.ct_utils import sort_dictionary_by_value\n",
    "from megadetector.utils.path_utils import parallel_copy_files\n",
    "from megadetector.utils.path_utils import recursive_file_list\n",
    "from megadetector.utils.split_locations_into_train_val import split_locations_into_train_val\n",
    "from megadetector.visualization.visualization_utils import resize_image_folder\n",
    "\n",
    "DATA_FOLDER = '/groups/cv-workshop/ohio_small_animals'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c232f16-902d-4fd5-8a08-9f2e6ed072ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_file = os.path.join(DATA_FOLDER, 'osu-small-animals.json')\n",
    "\n",
    "with open(metadata_file,'r') as f:\n",
    "    image_info = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1faf4fe7-2d5a-455b-b474-2356de7d6dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded information for 118554 images\n"
     ]
    }
   ],
   "source": [
    "print('Loaded information for {} images'.format(len(image_info['images'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af6367d-d95f-4f2b-9d09-0537635c166e",
   "metadata": {},
   "source": [
    "Print out the counts per category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f0f1a23-c41b-4380-943d-ecc943a9a5d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3e752e3a27a479c89508e5f2c5d144f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/118554 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eastern_gartersnake: 31899\n",
      "song_sparrow: 14567\n",
      "meadow_vole: 14169\n",
      "empty: 11448\n",
      "white-footed_mouse: 10548\n",
      "northern_house_wren: 5934\n",
      "invertebrate: 5075\n",
      "common_five-lined_skink: 5045\n",
      "masked_shrew: 4242\n",
      "eastern_cottontail: 3263\n",
      "long-tailed_weasel: 2325\n",
      "woodland_jumping_mouse: 1510\n",
      "plains_gartersnake: 1272\n",
      "eastern_massasauga: 1189\n",
      "virginia_opossum: 985\n",
      "common_yellowthroat: 802\n",
      "n._short-tailed_shrew: 746\n",
      "dekay's_brownsnake: 529\n",
      "american_mink: 425\n",
      "american_toad: 340\n",
      "eastern_racer_snake: 293\n",
      "smooth_greensnake: 264\n",
      "eastern_chipmunk: 198\n",
      "northern_leopard_frog: 193\n",
      "meadow_jumping_mouse: 160\n",
      "butler's_gartersnake: 155\n",
      "eastern_ribbonsnake: 133\n",
      "northern_watersnake: 121\n",
      "star-nosed_mole: 111\n",
      "striped_skunk: 104\n",
      "eastern_milksnake: 72\n",
      "gray_ratsnake: 68\n",
      "eastern_hog-nosed_snake: 67\n",
      "raccoon: 62\n",
      "green_frog: 47\n",
      "woodchuck: 44\n",
      "kirtland's_snake: 44\n",
      "indigo_bunting: 23\n",
      "painted_turtle: 23\n",
      "sora: 13\n",
      "american_bullfrog: 12\n",
      "gray_catbird: 10\n",
      "red-bellied_snake: 9\n",
      "brown_rat: 8\n",
      "snapping_turtle: 6\n",
      "eastern_bluebird: 1\n"
     ]
    }
   ],
   "source": [
    "image_id_to_annotations = defaultdict(list)\n",
    "\n",
    "category_id_to_name = {c['id']:c['name'] for c in image_info['categories']}\n",
    "category_name_to_id = {c['name']:c['id'] for c in image_info['categories']}\n",
    "\n",
    "category_name_to_count = defaultdict(int)\n",
    "\n",
    "for ann in image_info['annotations']:\n",
    "    image_id_to_annotations[ann['image_id']].append(ann)\n",
    "\n",
    "image_id_to_categories = defaultdict(set)\n",
    "\n",
    "for im in tqdm(image_info['images']):\n",
    "\n",
    "    annotations_this_image = image_id_to_annotations[im['id']]\n",
    "\n",
    "    if len(annotations_this_image) == 0:\n",
    "        image_id_to_categories[im['id']].add('unannotated')        \n",
    "    else:\n",
    "        for ann in annotations_this_image:\n",
    "            category_name = category_id_to_name[ann['category_id']]\n",
    "            image_id_to_categories[ann['image_id']].add(category_name)\n",
    "            category_name_to_count[category_name] += 1\n",
    "\n",
    "category_name_to_count = sort_dictionary_by_value(category_name_to_count,reverse=True)\n",
    "\n",
    "for category_name in category_name_to_count:\n",
    "    print('{}: {}'.format(category_name,category_name_to_count[category_name]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478385da-2cd9-440e-b4e1-e3135dda124b",
   "metadata": {},
   "source": [
    "Select categories that have more than 100 labeled examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9281c864-32eb-457f-b8ea-6bf1066fc3ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excluding category eastern_milksnake\n",
      "Excluding category gray_ratsnake\n",
      "Excluding category eastern_hog-nosed_snake\n",
      "Excluding category raccoon\n",
      "Excluding category green_frog\n",
      "Excluding category woodchuck\n",
      "Excluding category kirtland's_snake\n",
      "Excluding category indigo_bunting\n",
      "Excluding category painted_turtle\n",
      "Excluding category sora\n",
      "Excluding category american_bullfrog\n",
      "Excluding category gray_catbird\n",
      "Excluding category red-bellied_snake\n",
      "Excluding category brown_rat\n",
      "Excluding category snapping_turtle\n",
      "Excluding category eastern_bluebird\n"
     ]
    }
   ],
   "source": [
    "raw_category_to_target_category = {}\n",
    "\n",
    "min_images_to_include_in_training = 100\n",
    "\n",
    "for raw_category_name in category_name_to_count:\n",
    "    if category_name_to_count[raw_category_name] > min_images_to_include_in_training:\n",
    "        raw_category_to_target_category[raw_category_name] = raw_category_name\n",
    "    else:\n",
    "        print('Excluding category {}'.format(raw_category_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab49321c-9cfd-47a9-8509-2a8a24b609a7",
   "metadata": {},
   "source": [
    "These COCO annotation files also have location metadata. Count the animals from each location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db48b854-ed5f-48eb-8d9e-a78beb705667",
   "metadata": {},
   "outputs": [],
   "source": [
    "location_to_category_counts = {}\n",
    "category_to_location_counts = {}\n",
    "\n",
    "all_locations = set()\n",
    "\n",
    "for im in image_info['images']:\n",
    "\n",
    "    location = im['location']\n",
    "    all_locations.add(location)\n",
    "    categories_this_image = image_id_to_categories[im['id']]\n",
    "\n",
    "    if location not in location_to_category_counts:\n",
    "        location_to_category_counts[location] = {}\n",
    "\n",
    "    for raw_category_name in categories_this_image:\n",
    "\n",
    "        if raw_category_name not in raw_category_to_target_category:\n",
    "            continue\n",
    "        category_name = raw_category_to_target_category[raw_category_name]\n",
    "\n",
    "        if category_name not in category_to_location_counts:\n",
    "            category_to_location_counts[category_name] = {}\n",
    "        \n",
    "        if location not in category_to_location_counts[category_name]:\n",
    "            category_to_location_counts[category_name][location] = 0\n",
    "\n",
    "        if category_name not in location_to_category_counts[location]:\n",
    "            location_to_category_counts[location][category_name] = 0\n",
    "            \n",
    "        location_to_category_counts[location][category_name] = \\\n",
    "            location_to_category_counts[location][category_name] + 1\n",
    "    \n",
    "        category_to_location_counts[category_name][location] = \\\n",
    "            category_to_location_counts[category_name][location] + 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f483d6-b4a8-4dc7-920e-1193494cb653",
   "metadata": {},
   "source": [
    "## Create training and validation\n",
    "In earlier notebooks, you were given premade training and validation datasets. Recall, the training data is used for tuning your model and the validation is held out for testing. Come up with a training and validation split for your model. This could be purely random or based on some relevant metadata. Up to you!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5860b52-cd94-4a8c-b839-ec52201273f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting 30 categories over 168 locations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:05<00:00, 1668.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "47 of 10000 random seeds satisfied hard constraints\n",
      "\n",
      "Val locations:\n",
      "\n",
      "DORRS\n",
      "MONC295N\n",
      "GRN2\n",
      "KILC3W\n",
      "KILD1E\n",
      "KILC5WA\n",
      "GNR1\n",
      "FCM2\n",
      "MLN2B\n",
      "KILQ1S\n",
      "CBNP1S\n",
      "FCP1\n",
      "KILJ14W\n",
      "KILB11W\n",
      "KPC3\n",
      "KILH10S\n",
      "KILQ3S\n",
      "KILC1E\n",
      "RMW1\n",
      "KILD1W\n",
      "KILF10S\n",
      "KPA2\n",
      "KILE1W\n",
      "WIL2\n",
      "KILC2S\n",
      "\n",
      "Val fractions by category:\n",
      "\n",
      "eastern_gartersnake (31899) 0.12\n",
      "song_sparrow (14567) 0.09\n",
      "meadow_vole (14169) 0.08\n",
      "empty (11448) 0.10\n",
      "white-footed_mouse (10548) 0.17\n",
      "northern_house_wren (5934) 0.21\n",
      "invertebrate (5075) 0.11\n",
      "common_five-lined_skink (5045) 0.07\n",
      "masked_shrew (4242) 0.17\n",
      "eastern_cottontail (3263) 0.17\n",
      "long-tailed_weasel (2325) 0.14\n",
      "woodland_jumping_mouse (1510) 0.17\n",
      "plains_gartersnake (1272) 0.07\n",
      "eastern_massasauga (1189) 0.15\n",
      "virginia_opossum (985) 0.11\n",
      "common_yellowthroat (802) 0.27\n",
      "n._short-tailed_shrew (746) 0.13\n",
      "dekay's_brownsnake (529) 0.17\n",
      "american_mink (425) 0.03\n",
      "american_toad (340) 0.10\n",
      "eastern_racer_snake (293) 0.13\n",
      "smooth_greensnake (264) 0.10\n",
      "eastern_chipmunk (198) 0.12\n",
      "northern_leopard_frog (193) 0.12\n",
      "meadow_jumping_mouse (160) 0.25\n",
      "butler's_gartersnake (155) 0.34\n",
      "eastern_ribbonsnake (133) 0.30\n",
      "northern_watersnake (121) 0.17\n",
      "star-nosed_mole (111) 0.19\n",
      "striped_skunk (104) 0.11\n",
      "Assigned 25 of 168 locations to the val split\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# split on location\n",
    "\n",
    "train_fraction = 0.85\n",
    "\n",
    "train_on_crops = False\n",
    "random.seed(0)\n",
    "\n",
    "val_locations,category_to_val_fraction = \\\n",
    "      split_locations_into_train_val(location_to_category_counts,\n",
    "                                     n_random_seeds=10000,\n",
    "                                     target_val_fraction=1.0-train_fraction,\n",
    "                                     category_to_max_allowable_error=None,                                   \n",
    "                                     category_to_error_weight=None,\n",
    "                                     default_max_allowable_error=0.2,\n",
    "                                     require_complete_coverage=True)\n",
    "\n",
    "print('Assigned {} of {} locations to the val split'.format(len(val_locations),len(all_locations)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22cb41da-5471-4700-bc09-9e8f7b7a094f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write these splits out\n",
    "ptf = '/noc/users/ericor/computer-vision-workshop/object_detection'\n",
    "\n",
    "train_locations = []\n",
    "for location in all_locations:\n",
    "    if location not in val_locations:\n",
    "        train_locations.append(location)\n",
    "\n",
    "assert len(train_locations) + len(val_locations) == len(all_locations)\n",
    "\n",
    "split_file = os.path.join(ptf,'location_splits.json')\n",
    "\n",
    "split_info = {}\n",
    "split_info['train'] = sorted(list(train_locations))\n",
    "split_info['val'] = sorted(list(val_locations))\n",
    "\n",
    "with open(split_file,'w') as f:\n",
    "    json.dump(split_info,f,indent=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d363d3b2-17c9-4539-907a-d725ba58a1a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignored 0 images with multiple categories\n",
      "Ignored 509 images with excluded categories\n",
      "Before sampling:\n",
      "\n",
      "empty: 11448\n",
      "american_toad: 340\n",
      "northern_leopard_frog: 193\n",
      "common_yellowthroat: 802\n",
      "northern_house_wren: 5934\n",
      "song_sparrow: 14567\n",
      "invertebrate: 5075\n",
      "common_five-lined_skink: 5045\n",
      "american_mink: 425\n",
      "eastern_chipmunk: 198\n",
      "eastern_cottontail: 3263\n",
      "long-tailed_weasel: 2325\n",
      "masked_shrew: 4242\n",
      "meadow_jumping_mouse: 160\n",
      "meadow_vole: 14169\n",
      "n._short-tailed_shrew: 746\n",
      "star-nosed_mole: 111\n",
      "striped_skunk: 104\n",
      "virginia_opossum: 985\n",
      "white-footed_mouse: 10548\n",
      "woodland_jumping_mouse: 1510\n",
      "butler's_gartersnake: 155\n",
      "dekay's_brownsnake: 529\n",
      "eastern_gartersnake: 31899\n",
      "eastern_massasauga: 1189\n",
      "eastern_racer_snake: 293\n",
      "eastern_ribbonsnake: 133\n",
      "northern_watersnake: 121\n",
      "plains_gartersnake: 1272\n",
      "smooth_greensnake: 264\n"
     ]
    }
   ],
   "source": [
    "# Assign images to unique categories\n",
    "category_name_to_images = defaultdict(list)\n",
    "images_with_multiple_categories = []\n",
    "images_with_ignored_categories = []\n",
    "\n",
    "for im in image_info['images']:\n",
    "\n",
    "    categories_this_image = image_id_to_categories[im['id']]\n",
    "    assert len(categories_this_image) > 0\n",
    "    if len(categories_this_image) > 1:\n",
    "        images_with_multiple_categories.append([im['file_name'],categories_this_image])\n",
    "        continue\n",
    "    category_name = list(categories_this_image)[0]\n",
    "    if category_name not in raw_category_to_target_category:\n",
    "        images_with_ignored_categories.append(im['file_name'])\n",
    "        continue\n",
    "    category_name_to_images[category_name].append(im)\n",
    "\n",
    "print('Ignored {} images with multiple categories'.format(len(images_with_multiple_categories)))   \n",
    "print('Ignored {} images with excluded categories'.format(len(images_with_ignored_categories)))\n",
    "\n",
    "print('Before sampling:\\n')\n",
    "for category_name in category_name_to_images:\n",
    "    print('{}: {}'.format(category_name,\n",
    "                          len(category_name_to_images[category_name])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f12fe943-0552-4542-a45d-4d30075b7fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After sampling:\n",
      "\n",
      "empty: 11448\n",
      "american_toad: 340\n",
      "northern_leopard_frog: 193\n",
      "common_yellowthroat: 802\n",
      "northern_house_wren: 5934\n",
      "song_sparrow: 14567\n",
      "invertebrate: 5075\n",
      "common_five-lined_skink: 5045\n",
      "american_mink: 425\n",
      "eastern_chipmunk: 198\n",
      "eastern_cottontail: 3263\n",
      "long-tailed_weasel: 2325\n",
      "masked_shrew: 4242\n",
      "meadow_jumping_mouse: 160\n",
      "meadow_vole: 14169\n",
      "n._short-tailed_shrew: 746\n",
      "star-nosed_mole: 111\n",
      "striped_skunk: 104\n",
      "virginia_opossum: 985\n",
      "white-footed_mouse: 10548\n",
      "woodland_jumping_mouse: 1510\n",
      "butler's_gartersnake: 155\n",
      "dekay's_brownsnake: 529\n",
      "eastern_gartersnake: 31899\n",
      "eastern_massasauga: 1189\n",
      "eastern_racer_snake: 293\n",
      "eastern_ribbonsnake: 133\n",
      "northern_watersnake: 121\n",
      "plains_gartersnake: 1272\n",
      "smooth_greensnake: 264\n"
     ]
    }
   ],
   "source": [
    "# enforce a max of blank samples\n",
    "# max_blanks = 20000\n",
    "max_blanks = 200000\n",
    "\n",
    "if len(category_name_to_images['empty']) > max_blanks:\n",
    "    category_name_to_images['empty'] = random.sample(category_name_to_images['empty'],k=max_blanks)\n",
    "\n",
    "print('After sampling:\\n')\n",
    "for category_name in category_name_to_images:\n",
    "    print('{}: {}'.format(category_name,\n",
    "                          len(category_name_to_images[category_name])))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af14fbe5-1728-4b2e-add4-3d2b0b3a56e9",
   "metadata": {},
   "source": [
    "## Train your model\n",
    "Train a classification model based on the COCO formatted data. You can try to write a custom DataLoader to read in COCO data for classification (as opposed to object detection). Or you could try a different architecture all together. [Ultralytics](https://docs.ultralytics.com/tasks/classify/#models), for example, distributes classification models that can read in data from COCO formatted metadata (NB: this is a different tool from Pytorch and will require installing a new package). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d07a0f-e4b9-4c8b-aadb-49940664bd3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv-workshop",
   "language": "python",
   "name": "cv-workshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
